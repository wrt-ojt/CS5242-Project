{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"W3UTCc1cfSb0"},"outputs":[],"source":["import os\n","import pandas as pd\n","from PIL import Image\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision import transforms\n","from transformers import CLIPProcessor, CLIPModel\n","from sklearn.metrics import accuracy_score, classification_report\n","from tqdm.auto import tqdm # For progress bars"]},{"cell_type":"code","source":["CONFIG = {\n","    \"data_dir\": \"data\", # Directory containing images and texts\n","    \"label_file\": \"label.csv\", # CSV file with 'id' and 'label' columns\n","    \"clip_model_name\": \"openai/clip-vit-base-patch32\", # Or other CLIP model\n","    \"num_classes\": 3, # positive, negative, neutral\n","    \"batch_size\": 16, # Adjust based on GPU memory\n","    \"learning_rate_clip\": 1e-6, # Smaller LR for pre-trained CLIP\n","    \"learning_rate_head\": 1e-4, # Larger LR for custom head\n","    \"num_epochs\": 10, # Number of training epochs\n","    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n","    \"val_split_ratio\": 0.15, # Validation set ratio\n","    \"test_split_ratio\": 0.15, # Test set ratio\n","    \"seed\": 42, # For reproducible splits/shuffling\n","    \"max_token_length\": 77, # Standard CLIP context length\n","    # --- Ablation Study Flags ---\n","    \"use_cross_attention\": True,\n","    \"use_cnn_layer\": True,\n","    \"freeze_clip\": False # Set to True to freeze CLIP weights initially\n","}"],"metadata":{"id":"zeB3NJ4UfVb9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Label mapping\n","label_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n","# Inverse mapping for reporting\n","inv_label_map = {v: k for k, v in label_map.items()}"],"metadata":{"id":"CB9Y_ATNfVxk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 2. Dataset and DataLoader ---\n","class MultimodalBlogDataset(Dataset):\n","    \"\"\"Custom Dataset for loading image-text pairs.\"\"\"\n","    def __init__(self, data_dir, dataframe, clip_processor, label_map):\n","        self.data_dir = data_dir\n","        self.dataframe = dataframe\n","        self.processor = clip_processor\n","        self.label_map = label_map\n","        # Image transformations are handled by CLIPProcessor,\n","        # but ensure images are loaded correctly (RGB)\n","        self.image_loader = transforms.Compose([\n","            transforms.ToTensor() # ToTensor is needed before processor usually\n","                                  # Processor handles resize and normalize\n","        ])\n","\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        row = self.dataframe.iloc[idx]\n","        item_id = row['id']\n","        label_str = row['label']\n","        label = self.label_map[label_str]\n","\n","        # Load Image\n","        img_path = os.path.join(self.data_dir, f\"{item_id}.jpg\")\n","        try:\n","            image = Image.open(img_path).convert(\"RGB\")\n","        except FileNotFoundError:\n","            print(f\"Warning: Image file not found {img_path}, returning None.\")\n","            return None # Handle appropriately in collate_fn or dataloader\n","\n","        # Load Text\n","        txt_path = os.path.join(self.data_dir, f\"{item_id}.txt\")\n","        try:\n","            with open(txt_path, 'r', encoding='utf-8') as f:\n","                text = f.read()\n","        except FileNotFoundError:\n","            print(f\"Warning: Text file not found {txt_path}, returning None.\")\n","            return None # Handle appropriately\n","\n","        # Preprocessing is done in the training loop / collate_fn\n","        # Here we just return the raw data + label\n","        return image, text, label"],"metadata":{"id":"_UurpPsIfWEV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collate_fn(batch, processor, device, max_length):\n","    \"\"\"Custom collate function to handle preprocessing within the batch.\"\"\"\n","    # Filter out None items if any file was not found\n","    batch = [item for item in batch if item is not None]\n","    if not batch:\n","        return None\n","\n","    images, texts, labels = zip(*batch)\n","\n","    # Process batch using CLIPProcessor\n","    inputs = processor(\n","        text=list(texts),\n","        images=list(images),\n","        return_tensors=\"pt\",\n","        padding=\"max_length\", # Pad to max_length\n","        truncation=True,\n","        max_length=max_length\n","    )\n","\n","    # Move tensors to the correct device\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","    labels = torch.tensor(labels, dtype=torch.long).to(device)\n","\n","    return inputs, labels"],"metadata":{"id":"6ub2AAuxgBkF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 3. Model Architecture ---\n","class MultimodalClassifier(nn.Module):\n","    \"\"\"The main model combining CLIP features with a custom fusion head.\"\"\"\n","    def __init__(self, clip_model_name, num_classes,\n","                 use_cross_attention=True, use_cnn_layer=True, freeze_clip=False, device='cpu'):\n","        super().__init__()\n","        self.use_cross_attention = use_cross_attention\n","        self.use_cnn_layer = use_cnn_layer\n","        self.device = device\n","\n","        # Load CLIP model\n","        self.clip_model = CLIPModel.from_pretrained(clip_model_name).to(self.device)\n","\n","        # Freeze CLIP weights if specified\n","        if freeze_clip:\n","            print(\"Freezing CLIP model parameters.\")\n","            for param in self.clip_model.parameters():\n","                param.requires_grad = False\n","        else:\n","            print(\"CLIP model parameters will be fine-tuned.\")\n","\n","\n","        # Get CLIP embedding dimension (projection_dim)\n","        self.embed_dim = self.clip_model.projection_dim # e.g., 512 or 768\n","\n","        # --- Fusion Layers ---\n","        if self.use_cross_attention:\n","            # MultiheadAttention expects (batch, seq_len, embed_dim) if batch_first=True\n","            # Our features are (batch, embed_dim), so add seq_len=1\n","            self.img_to_txt_attention = nn.MultiheadAttention(self.embed_dim, num_heads=8, batch_first=True, dropout=0.1)\n","            self.txt_to_img_attention = nn.MultiheadAttention(self.embed_dim, num_heads=8, batch_first=True, dropout=0.1)\n","            fusion_input_dim = self.embed_dim * 4 # img_feat + txt_feat + attended_img + attended_txt\n","        else:\n","            fusion_input_dim = self.embed_dim * 2 # img_feat + txt_feat\n","\n","        # --- CNN Layer (Optional) ---\n","        # Applying Conv1d on concatenated features of length 1.\n","        # Kernel size 1 acts like a Linear layer applied independently to each channel.\n","        # Might not capture \"global perception\" in the traditional sense here.\n","        if self.use_cnn_layer:\n","            self.cnn_out_channels = fusion_input_dim // 2 # Example reduction\n","            # Input shape for Conv1d: (batch, channels, length)\n","            # Our concatenated features: (batch, fusion_input_dim)\n","            # Reshape to: (batch, fusion_input_dim, 1)\n","            self.conv1d = nn.Conv1d(in_channels=fusion_input_dim,\n","                                    out_channels=self.cnn_out_channels,\n","                                    kernel_size=1, # Acts like a linear projection per channel\n","                                    padding=0)\n","            self.relu_cnn = nn.ReLU()\n","            # After Conv1d: (batch, cnn_out_channels, 1) -> Flatten -> (batch, cnn_out_channels)\n","            classifier_input_dim = self.cnn_out_channels\n","        else:\n","            classifier_input_dim = fusion_input_dim # Input dim for MLP if CNN is skipped\n","\n","        # --- Classifier Head (MLP: Increase then Decrease Dim) ---\n","        self.classifier_hidden_dim = classifier_input_dim * 2 # \"升维\"\n","        self.fc1 = nn.Linear(classifier_input_dim, self.classifier_hidden_dim)\n","        self.relu_fc1 = nn.ReLU()\n","        self.dropout1 = nn.Dropout(0.3)\n","        self.fc2 = nn.Linear(self.classifier_hidden_dim, num_classes) # \"降维\" to num_classes\n","\n","\n","    def forward(self, inputs):\n","        # Get CLIP features\n","        # Note: Use **inputs to unpack dict directly into arguments\n","        image_features = self.clip_model.get_image_features(pixel_values=inputs['pixel_values'])\n","        text_features = self.clip_model.get_text_features(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n","        # Features are typically (batch_size, embed_dim)\n","\n","        # --- Fusion ---\n","        if self.use_cross_attention:\n","            # Reshape features for MultiheadAttention: (batch, seq_len=1, embed_dim)\n","            img_feat_attn = image_features.unsqueeze(1)\n","            txt_feat_attn = text_features.unsqueeze(1)\n","\n","            # Image attends to Text (Q=img, K=txt, V=txt)\n","            attended_img, _ = self.img_to_txt_attention(img_feat_attn, txt_feat_attn, txt_feat_attn)\n","            attended_img = attended_img.squeeze(1) # Back to (batch, embed_dim)\n","\n","            # Text attends to Image (Q=txt, K=img, V=img)\n","            attended_txt, _ = self.txt_to_img_attention(txt_feat_attn, img_feat_attn, img_feat_attn)\n","            attended_txt = attended_txt.squeeze(1) # Back to (batch, embed_dim)\n","\n","            # Concatenate all features\n","            fused_features = torch.cat([image_features, text_features, attended_img, attended_txt], dim=1)\n","            # Shape: (batch, embed_dim * 4)\n","        else:\n","            # Simple concatenation if cross-attention is disabled\n","            fused_features = torch.cat([image_features, text_features], dim=1)\n","            # Shape: (batch, embed_dim * 2)\n","\n","        # --- Optional CNN Layer ---\n","        if self.use_cnn_layer:\n","            # Reshape for Conv1d: (batch, channels=fusion_input_dim, length=1)\n","            cnn_input = fused_features.unsqueeze(2)\n","            cnn_output = self.conv1d(cnn_input)\n","            cnn_output = self.relu_cnn(cnn_output)\n","            # Flatten: (batch, cnn_out_channels, 1) -> (batch, cnn_out_channels)\n","            classifier_input = cnn_output.squeeze(2)\n","        else:\n","            classifier_input = fused_features # Pass concatenated features directly\n","\n","        # --- Classifier Head ---\n","        x = self.fc1(classifier_input)\n","        x = self.relu_fc1(x)\n","        x = self.dropout1(x)\n","        logits = self.fc2(x) # Output logits (batch, num_classes)\n","\n","        return logits"],"metadata":{"id":"eNMHEVjCgB3m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 4. Training and Evaluation Functions ---\n","\n","def train_epoch(model, dataloader, optimizer, criterion, device, clip_processor, max_length):\n","    model.train() # Set model to training mode\n","    total_loss = 0\n","    all_preds = []\n","    all_labels = []\n","\n","    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n","    for batch in progress_bar:\n","        if batch is None: continue # Skip if collate_fn returned None\n","        inputs, labels = batch\n","\n","        # Zero gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","\n","        # Calculate loss\n","        loss = criterion(outputs, labels)\n","\n","        # Backward pass and optimization\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","        # Store predictions and labels for metric calculation\n","        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n","        all_preds.extend(preds)\n","        all_labels.extend(labels.cpu().numpy())\n","\n","        progress_bar.set_postfix({'loss': loss.item()})\n","\n","    avg_loss = total_loss / len(dataloader)\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    return avg_loss, accuracy"],"metadata":{"id":"HKVITPimgCEG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_epoch(model, dataloader, criterion, device, clip_processor, max_length):\n","    model.eval() # Set model to evaluation mode\n","    total_loss = 0\n","    all_preds = []\n","    all_labels = []\n","\n","    progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n","    with torch.no_grad(): # Disable gradient calculations\n","        for batch in progress_bar:\n","            if batch is None: continue\n","            inputs, labels = batch\n","\n","            # Forward pass\n","            outputs = model(inputs)\n","\n","            # Calculate loss\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","\n","            # Store predictions and labels\n","            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n","            all_preds.extend(preds)\n","            all_labels.extend(labels.cpu().numpy())\n","            progress_bar.set_postfix({'loss': loss.item()})\n","\n","\n","    avg_loss = total_loss / len(dataloader)\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    report = classification_report(all_labels, all_preds, target_names=label_map.keys(), zero_division=0)\n","\n","    return avg_loss, accuracy, report, all_labels, all_preds"],"metadata":{"id":"nKgF3XPJgL1F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 5. Main Execution ---\n","print(f\"Using device: {CONFIG['device']}\")\n","torch.manual_seed(CONFIG['seed'])\n","\n","# --- Load Data ---\n","print(\"Loading labels...\")\n","try:\n","    df = pd.read_csv(CONFIG['label_file'])\n","    # Basic validation\n","    if 'id' not in df.columns or 'label' not in df.columns:\n","          raise ValueError(\"CSV must contain 'id' and 'label' columns.\")\n","    if not all(label in label_map for label in df['label'].unique()):\n","        raise ValueError(f\"Labels in CSV must be one of {list(label_map.keys())}\")\n","    print(f\"Found {len(df)} samples.\")\n","except FileNotFoundError:\n","    print(f\"Error: Label file not found at {CONFIG['label_file']}\")\n","    exit()\n","except ValueError as e:\n","    print(f\"Error: {e}\")\n","    exit()\n","\n","\n","# --- Split Data ---\n","# Calculate split sizes\n","total_size = len(df)\n","test_size = int(CONFIG['test_split_ratio'] * total_size)\n","val_size = int(CONFIG['val_split_ratio'] * total_size)\n","train_size = total_size - val_size - test_size\n","\n","print(f\"Splitting data: Train={train_size}, Val={val_size}, Test={test_size}\")\n","if train_size <= 0 or val_size <= 0 or test_size <= 0:\n","    print(\"Error: Dataset too small for specified split ratios.\")\n","    exit()\n","\n","# Perform the split\n","train_df, val_df, test_df = random_split(df, [train_size, val_size, test_size],\n","                                          generator=torch.Generator().manual_seed(CONFIG['seed']))\n","\n","# Convert subsets back to DataFrames for easier indexing if needed by Dataset class\n","# Note: random_split returns Subset objects. We get the indices and select from the original df.\n","train_df = df.iloc[train_df.indices].reset_index(drop=True)\n","val_df = df.iloc[val_df.indices].reset_index(drop=True)\n","test_df = df.iloc[test_df.indices].reset_index(drop=True)\n","\n","\n","# --- Initialize Processor, Dataset, DataLoader ---\n","print(\"Initializing CLIP Processor...\")\n","clip_processor = CLIPProcessor.from_pretrained(CONFIG['clip_model_name'])\n","\n","print(\"Creating Datasets...\")\n","train_dataset = MultimodalBlogDataset(CONFIG['data_dir'], train_df, clip_processor, label_map)\n","val_dataset = MultimodalBlogDataset(CONFIG['data_dir'], val_df, clip_processor, label_map)\n","test_dataset = MultimodalBlogDataset(CONFIG['data_dir'], test_df, clip_processor, label_map)\n","\n","print(\"Creating DataLoaders...\")\n","# Define the collate function with necessary arguments partially filled\n","collate_fn_partial = lambda batch: collate_fn(batch, clip_processor, CONFIG['device'], CONFIG['max_token_length'])\n","\n","train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, collate_fn=collate_fn_partial)\n","val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_fn_partial)\n","test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_fn_partial)\n","\n","\n","# --- Initialize Model, Loss, Optimizer ---\n","print(\"Initializing Model...\")\n","# Pass ablation flags here\n","model = MultimodalClassifier(\n","    clip_model_name=CONFIG['clip_model_name'],\n","    num_classes=CONFIG['num_classes'],\n","    use_cross_attention=CONFIG[\"use_cross_attention\"],\n","    use_cnn_layer=CONFIG[\"use_cnn_layer\"],\n","    freeze_clip=CONFIG[\"freeze_clip\"],\n","    device=CONFIG['device']\n",")\n","# model = model.to(CONFIG['device']) # Model parts are moved to device in __init__\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","# Separate parameters for different learning rates\n","clip_params = list(model.clip_model.parameters())\n","head_params = []\n","if CONFIG[\"use_cross_attention\"]:\n","    head_params.extend(list(model.img_to_txt_attention.parameters()))\n","    head_params.extend(list(model.txt_to_img_attention.parameters()))\n","if CONFIG[\"use_cnn_layer\"]:\n","    head_params.extend(list(model.conv1d.parameters()))\n","head_params.extend(list(model.fc1.parameters()))\n","head_params.extend(list(model.fc2.parameters()))\n","\n","\n","optimizer = optim.AdamW([\n","    {'params': clip_params, 'lr': CONFIG['learning_rate_clip']},\n","    {'params': head_params, 'lr': CONFIG['learning_rate_head']}\n","])\n","\n","# --- Training Loop ---\n","print(\"Starting Training...\")\n","best_val_accuracy = 0.0\n","best_epoch = -1\n","\n","for epoch in range(CONFIG['num_epochs']):\n","    print(f\"\\n--- Epoch {epoch+1}/{CONFIG['num_epochs']} ---\")\n","\n","    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, CONFIG['device'], clip_processor, CONFIG['max_token_length'])\n","    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n","\n","    val_loss, val_acc, val_report, _, _ = evaluate_epoch(model, val_loader, criterion, CONFIG['device'], clip_processor, CONFIG['max_token_length'])\n","    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n","    print(\"Validation Classification Report:\\n\", val_report)\n","\n","    # Save best model based on validation accuracy\n","    if val_acc > best_val_accuracy:\n","        best_val_accuracy = val_acc\n","        best_epoch = epoch\n","        # Create a directory to save models if it doesn't exist\n","        os.makedirs(\"models\", exist_ok=True)\n","        model_save_path = os.path.join(\"models\", \"best_multimodal_model.pth\")\n","        print(f\"Validation accuracy improved. Saving model to {model_save_path}\")\n","        torch.save(model.state_dict(), model_save_path)\n","        # You might want to save optimizer state and epoch number too for resuming training\n","\n","print(f\"\\nTraining finished. Best validation accuracy ({best_val_accuracy:.4f}) achieved at epoch {best_epoch+1}.\")\n","\n","# --- Final Evaluation on Test Set ---\n","print(\"\\n--- Evaluating on Test Set using Best Model ---\")\n","# Load the best model weights\n","best_model_path = os.path.join(\"models\", \"best_multimodal_model.pth\")\n","if os.path.exists(best_model_path):\n","    model.load_state_dict(torch.load(best_model_path, map_location=CONFIG['device']))\n","    print(\"Loaded best model weights for testing.\")\n","\n","    test_loss, test_acc, test_report, test_labels, test_preds = evaluate_epoch(model, test_loader, criterion, CONFIG['device'], clip_processor, CONFIG['max_token_length'])\n","    print(f\"\\nTest Loss: {test_loss:.4f}\")\n","    print(f\"Test Accuracy: {test_acc:.4f}\")\n","    print(\"Test Set Classification Report:\\n\", test_report)\n","    # You can further analyze test_labels and test_preds here (e.g., confusion matrix)\n","else:\n","    print(\"Warning: Best model file not found. Skipping test set evaluation.\")\n","\n","print(\"\\nDone.\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"id":"ZQw_-TaHgLyf","executionInfo":{"status":"error","timestamp":1743952190143,"user_tz":-480,"elapsed":281,"user":{"displayName":"叶芳达","userId":"17538328334288231675"}},"outputId":"5ac8fc3c-8e78-4dec-fa9d-ce7261800e48"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","Loading labels...\n","Error: Label file not found at label.csv\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'df' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-b50c1c435377>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# --- Split Data ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Calculate split sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtotal_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_split_ratio'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtotal_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mval_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_split_ratio'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtotal_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"7vP5jW4WgLr_"},"execution_count":null,"outputs":[]}]}