{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNxDAUAGzMDaGPpQ3ZyTsvy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"6a3594b79646436c9e13ad498b323e78":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4d0141dd46134a529585b5a010d78026","IPY_MODEL_8035bdc1d74d454db52c2b169d0ede3e","IPY_MODEL_e9394a3cb8a547b5926072a9f7488201"],"layout":"IPY_MODEL_94ab9d4a658643bab676e9a60e059bbf"}},"4d0141dd46134a529585b5a010d78026":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_970a5d42c5a6431a99e1d6c59f3d0489","placeholder":"​","style":"IPY_MODEL_676cbdf1126347ee9af4844190d4843a","value":"Training:  70%"}},"8035bdc1d74d454db52c2b169d0ede3e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_5df9408958a3430ca93b6c00c811b89e","max":198,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b4e5d37110d749719b55c2d7da2fef7e","value":139}},"e9394a3cb8a547b5926072a9f7488201":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a586aab4e28431c88655eae4550ad78","placeholder":"​","style":"IPY_MODEL_4940f63579304bb0bd6336155c6e402e","value":" 139/198 [23:08&lt;08:53,  9.04s/it, loss=1.22]"}},"94ab9d4a658643bab676e9a60e059bbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"970a5d42c5a6431a99e1d6c59f3d0489":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"676cbdf1126347ee9af4844190d4843a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5df9408958a3430ca93b6c00c811b89e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4e5d37110d749719b55c2d7da2fef7e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0a586aab4e28431c88655eae4550ad78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4940f63579304bb0bd6336155c6e402e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n8-lPkzOEH3g","executionInfo":{"status":"ok","timestamp":1744004194124,"user_tz":-480,"elapsed":28010,"user":{"displayName":"Longfei Ju","userId":"03330376312878383448"}},"outputId":"e9b51a12-8d32-4519-a1cc-258a3ff212fe"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!ls /content/drive/MyDrive/ColabNotebooks/5242GP/CS5242_Project\n","#!ls /content/drive/MyDrive/ColabNotebooks/5242GP/CS5242_Project/image\n","#!ls /content/drive/MyDrive/ColabNotebooks/5242GP/CS5242_Project/preprocess/wsb/data_processed"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v_QgoehHG2vV","executionInfo":{"status":"ok","timestamp":1743965336617,"user_tz":-480,"elapsed":34098,"user":{"displayName":"Longfei Ju","userId":"03330376312878383448"}},"outputId":"04b15191-631f-495a-d9f6-7b728da65791"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cnn_featuers.gsheet  image  label.csv  MVSA_Single  Network  preprocess\n","^C\n","^C\n"]}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C6psKR1nE-FW","executionInfo":{"status":"ok","timestamp":1743959750033,"user_tz":-480,"elapsed":104,"user":{"displayName":"Longfei Ju","userId":"03330376312878383448"}},"outputId":"97353639-5332-411a-bcf9-a37f1104beeb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ls: cannot access '/content/drive/MyDrive/ColabNotebooks/5242GP/CS5242_Project': No such file or directory\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"2Ldqn0wvE76d","executionInfo":{"status":"ok","timestamp":1744004215656,"user_tz":-480,"elapsed":16789,"user":{"displayName":"Longfei Ju","userId":"03330376312878383448"}}},"outputs":[],"source":["import os\n","import pandas as pd\n","from PIL import Image\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision import transforms\n","from transformers import CLIPProcessor, CLIPModel  # Huggingface CLIP\n","from sklearn.metrics import accuracy_score, classification_report\n","from tqdm.auto import tqdm # For progress bars\n","import torch.nn.functional as F"]},{"cell_type":"markdown","source":["# --- 1. Configuration ---"],"metadata":{"id":"-Nj7-unbFd2y"}},{"cell_type":"code","source":["CONFIG = {\n","\n","    \"batch_size\": 16, # Adjust based on GPU memory\n","    \"clip_model_name\": \"openai/clip-vit-base-patch32\", # Or other CLIP model\n","    \"data_dir\": \"data\", # Directory containing images and texts\n","\n","    \"data_dir_img\":\"/content/drive/MyDrive/ColabNotebooks/5242GP/CS5242_Project/image\",\n","    \"data_dir_text\":\"/content/drive/MyDrive/ColabNotebooks/5242GP/CS5242_Project/preprocess/wsb/data_processed\",\n","\n","    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n","    \"freeze_clip\": True, # Set to True to freeze CLIP weights initially\n","    \"label_file\": \"/content/drive/MyDrive/ColabNotebooks/5242GP/label.csv\", # CSV file with 'id' and 'label' columns\n","    \"learning_rate_clip\": 1e-6, # Smaller LR for pre-trained CLIP\n","    \"learning_rate_head\": 1e-4, # Larger LR for custom head\n","    \"max_token_length\": 77, # Standard CLIP context length\n","    \"num_classes\": 3, # positive, negative, neutral\n","    \"num_epochs\": 10, # Number of training epochs\n","    \"seed\": 42, # For reproducible splits/shuffling\n","    \"test_split_ratio\": 0.15, # Test set ratio\n","    \"use_cross_attention\": False,\n","    \"use_cnn_layer\": True,\n","    \"val_split_ratio\": 0.15 # Validation set ratio\n","    # --- Ablation Study Flags ---\n","\n","\n","}"],"metadata":{"id":"3Ynt7yOXFcD0","executionInfo":{"status":"ok","timestamp":1744006375820,"user_tz":-480,"elapsed":14,"user":{"displayName":"Longfei Ju","userId":"03330376312878383448"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Label mapping\n","label_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n","# Inverse mapping for reporting\n","inv_label_map = {v: k for k, v in label_map.items()}"],"metadata":{"id":"gQlQgtPCFnj5","executionInfo":{"status":"ok","timestamp":1744004221152,"user_tz":-480,"elapsed":7,"user":{"displayName":"Longfei Ju","userId":"03330376312878383448"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# --- 2. Dataset and DataLoader ---"],"metadata":{"id":"kUBaTAQ-Ft2T"}},{"cell_type":"code","source":["class MultimodalBlogDataset(Dataset):\n","    \"\"\"Custom Dataset for loading image-text pairs.\"\"\"\n","    def __init__(self, data_dir, dataframe, clip_processor, label_map, data_dir_img, data_dir_text):\n","        self.data_dir = data_dir\n","\n","        ##### 建议回头text和img数据放到一个文件夹\n","        self.data_dir_img = data_dir_img\n","        self.data_dir_text = data_dir_text\n","        ####\n","\n","        self.dataframe = dataframe\n","        self.processor = clip_processor\n","        self.label_map = label_map\n","        # Image transformations are handled by CLIPProcessor,\n","        # but ensure images are loaded correctly (RGB)\n","        self.image_loader = transforms.Compose([\n","            transforms.ToTensor() # ToTensor is needed before processor usually\n","                                  # Processor handles resize and normalize\n","        ])\n","        #transforms不是transformer， 而是图片预处理的工具\n","        #totensor()把 PIL.Image 或 numpy.ndarray 类型的图像转化为torch.Tensor，并归一化(0,255)->(0,1)\n","        # .Compose 定义一个pipeline， 这里暂时只有totensor一个操作\n","        # 某些版本的CLIPProcessor只支持输入tensor\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        row = self.dataframe.iloc[idx]\n","        item_id = row['ID']\n","        label_str = row['label']\n","        label = self.label_map[label_str]\n","\n","        # Load Image\n","        img_path = os.path.join(self.data_dir_img, f\"{item_id}.jpg\")\n","        try:\n","            image = Image.open(img_path).convert(\"RGB\") #.jpg, .png可以忽略.convert(\"RGB\")\n","        except FileNotFoundError:\n","            print(f\"Warning: Image file not found {img_path}, returning None.\")\n","            return None # Handle appropriately in collate_fn or dataloader\n","\n","        # Load Text\n","        txt_path = os.path.join(self.data_dir_text, f\"{item_id}.txt\") #假设text文件和img文件放在一起\n","        try:\n","            with open(txt_path, 'r', encoding='utf-8') as f:\n","                text = f.read()\n","        except FileNotFoundError:\n","            print(f\"Warning: Text file not found {txt_path}, returning None.\")\n","            return None # Handle appropriately\n","\n","        # Preprocessing is done in the training loop / collate_fn\n","        # Here we just return the raw data + label\n","        return image, text, label\n"],"metadata":{"id":"VxFbSVFqFtDZ","executionInfo":{"status":"ok","timestamp":1744004222129,"user_tz":-480,"elapsed":2,"user":{"displayName":"Longfei Ju","userId":"03330376312878383448"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def collate_fn(batch, processor, device, max_length):\n","    \"\"\"Custom collate function to handle preprocessing within the batch.\"\"\"\n","    # Filter out None items if any file was not found\n","    batch = [item for item in batch if item is not None]\n","    if not batch:\n","        return None\n","\n","    images, texts, labels = zip(*batch) #将[img,text,label]数据转变为[img1, img2,..] [text1,text2..][label1,label2..]\n","\n","    # Process batch using CLIPProcessor\n","    inputs = processor(\n","        text=list(texts), #Huggingface版本的处理器传入list，和open ai版本不同\n","        images=list(images),\n","        return_tensors=\"pt\", #pytorch格式的tensor\n","        padding=\"max_length\", # Pad to max_length\n","        truncation=True,\n","        max_length=max_length\n","    )\n","    '''\n","    {\n","      'input_ids': tensor([...]),  不是词向量，而是词向量在词表中的id\n","      'attention_mask': tensor([...]), 全是0和1，告诉attention不要使用padding\n","      'pixel_values': tensor([...]) 图片resize后的tensor[B, 3, 224, 224] BCWH，不是encoder处理过的表征\n","    }\n","    '''\n","\n","    # Move tensors to the correct device\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","    #for k, v in inputs.items():\n","    #  print(f\"{k}: {v.device}\")\n","    labels = torch.tensor(labels, dtype=torch.long).to(device) #强制要求数据为长整型\n","\n","    return inputs, labels"],"metadata":{"id":"HyZZVgGNF1Yh","executionInfo":{"status":"ok","timestamp":1744004224979,"user_tz":-480,"elapsed":13,"user":{"displayName":"Longfei Ju","userId":"03330376312878383448"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# --- 3. Model Architecture ---"],"metadata":{"id":"6XV73NVNF3ia"}},{"cell_type":"code","source":["class MultimodalClassifier(nn.Module):\n","    \"\"\"The main model combining CLIP features with a custom fusion head.\"\"\"\n","    def __init__(self, clip_model_name, num_classes,processor,\n","                 use_cross_attention=True, use_cnn_layer=True, freeze_clip=True,  device='cpu'):\n","        super().__init__()\n","        self.use_cross_attention = use_cross_attention\n","        self.use_cnn_layer = use_cnn_layer\n","        self.device = device\n","\n","        # Load CLIP model\n","        self.clip_model = CLIPModel.from_pretrained(clip_model_name).to(self.device)\n","\n","        with torch.no_grad():\n","          emotion_prompts = [\n","              \"a photo and caption with negative emotion\",\n","              \"a photo and caption with neutral emotion\",\n","              \"a photo and caption with positive emotion\"\n","          ]\n","          text_inputs = processor(text=emotion_prompts, return_tensors=\"pt\").to(self.device)\n","          self.emotion_repr = self.clip_model.get_text_features(**text_inputs)\n","          self.emotion_repr = F.normalize(self.emotion_repr, dim=-1)  # optional\n","\n","        # Freeze CLIP weights if specified\n","        if freeze_clip: #open ai版本是默认无法调参的，hugging face版本支持fine tune\n","            print(\"Freezing CLIP model parameters.\")\n","            for param in self.clip_model.parameters():\n","                param.requires_grad = False\n","        else:\n","            print(\"CLIP model parameters will be fine-tuned.\")\n","\n","\n","        # Get CLIP embedding dimension (projection_dim)\n","        self.embed_dim = self.clip_model.projection_dim # e.g., 512 or 768\n","\n","        # --- Fusion Layers ---\n","        if self.use_cross_attention:\n","            # MultiheadAttention expects (batch, seq_len, embed_dim) if batch_first=True\n","            # Our features are (batch, embed_dim), so add seq_len=1\n","            self.img_to_txt_attention = nn.MultiheadAttention(self.embed_dim, num_heads=8, batch_first=True, dropout=0.1)\n","            self.txt_to_img_attention = nn.MultiheadAttention(self.embed_dim, num_heads=8, batch_first=True, dropout=0.1)\n","            fusion_input_dim = self.embed_dim * 4 # img_feat + txt_feat + attended_img + attended_txt\n","        else:\n","            fusion_input_dim = self.embed_dim * 2 # img_feat + txt_feat\n","\n","        self.cnn_hidden_channels = fusion_input_dim // 2 # Example reduction\n","        # Input shape for Conv1d: (batch, channels, length)\n","        # Our concatenated features: (batch, fusion_input_dim)\n","        # Reshape to: (batch, fusion_input_dim, 1)\n","        self.cnn = nn.Sequential(\n","            nn.Conv1d(in_channels=fusion_input_dim, out_channels=self.cnn_hidden_channels, kernel_size=1),\n","            nn.ReLU(),\n","            nn.Conv1d(in_channels=self.cnn_hidden_channels, out_channels=self.embed_dim, kernel_size=1)\n","        )\n","\n","\n","\n","    def forward(self, inputs):\n","        # Get CLIP features\n","        # Note: Use **inputs to unpack dict directly into arguments\n","        image_features = self.clip_model.get_image_features(pixel_values=inputs['pixel_values'])\n","        text_features = self.clip_model.get_text_features(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n","\n","        # Features are typically (batch_size, embed_dim)\n","\n","        # --- Fusion ---\n","        if self.use_cross_attention:\n","            # Reshape features for MultiheadAttention: (batch, seq_len=1, embed_dim)\n","            img_feat_attn = image_features.unsqueeze(1)\n","            txt_feat_attn = text_features.unsqueeze(1)\n","\n","            # Image attends to Text (Q=img, K=txt, V=txt)\n","            attended_img, _ = self.img_to_txt_attention(img_feat_attn, txt_feat_attn, txt_feat_attn)\n","            attended_img = attended_img.squeeze(1) # Back to (batch, embed_dim)\n","\n","            # Text attends to Image (Q=txt, K=img, V=img)\n","            attended_txt, _ = self.txt_to_img_attention(txt_feat_attn, img_feat_attn, img_feat_attn)\n","            attended_txt = attended_txt.squeeze(1) # Back to (batch, embed_dim)\n","\n","            # Concatenate all features\n","            fused_features = torch.cat([image_features, text_features, attended_img, attended_txt], dim=1)\n","            # Shape: (batch, embed_dim * 4)\n","        else:\n","            # Simple concatenation if cross-attention is disabled\n","            fused_features = torch.cat([image_features, text_features], dim=1)\n","            # Shape: (batch, embed_dim * 2)\n","\n","        cnn_input = fused_features.unsqueeze(-1)\n","        fused_repr = self.cnn(cnn_input)\n","        # Flatten: (batch, cnn_out_channels, 1) -> (batch, cnn_out_channels)\n","        fused_repr = fused_repr.squeeze(2)\n","\n","        #--- clip with emotion\n","        fused_repr = F.normalize(fused_repr, dim=-1)     # [B, D]\n","        #print(fused_repr.size())\n","\n","\n","        logits = 100.0 *fused_repr @ self.emotion_repr.T   # [B, 3] 100是温度系数，拉开差距\n","\n","        return logits"],"metadata":{"id":"1y_ku06kF4zk","executionInfo":{"status":"ok","timestamp":1744006398322,"user_tz":-480,"elapsed":2,"user":{"displayName":"Longfei Ju","userId":"03330376312878383448"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["# --- 4. Training and Evaluation Functions ---"],"metadata":{"id":"Ka0c6XiTF8S5"}},{"cell_type":"code","source":["def train_epoch(model, dataloader, optimizer, criterion, device, clip_processor, max_length):\n","    model.train() # Set model to training mode\n","    total_loss = 0\n","    all_preds = []\n","    all_labels = []\n","\n","    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n","    for batch in progress_bar:\n","        if batch is None: continue # Skip if collate_fn returned None\n","        inputs, labels = batch\n","\n","        # Zero gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","\n","        # Calculate loss\n","        loss = criterion(outputs, labels)\n","\n","        # Backward pass and optimization\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","        # Store predictions and labels for metric calculation\n","        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n","        all_preds.extend(preds)\n","        all_labels.extend(labels.cpu().numpy())\n","\n","        progress_bar.set_postfix({'loss': loss.item()})\n","\n","    avg_loss = total_loss / len(dataloader)\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    return avg_loss, accuracy\n","\n","def evaluate_epoch(model, dataloader, criterion, device, clip_processor, max_length):\n","    model.eval() # Set model to evaluation mode\n","    total_loss = 0\n","    all_preds = []\n","    all_labels = []\n","\n","    progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n","    with torch.no_grad(): # Disable gradient calculations\n","        for batch in progress_bar:\n","            if batch is None: continue\n","            inputs, labels = batch\n","\n","            # Forward pass\n","            outputs = model(inputs)\n","\n","            # Calculate loss\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","\n","            # Store predictions and labels\n","            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n","            all_preds.extend(preds)\n","            all_labels.extend(labels.cpu().numpy())\n","            progress_bar.set_postfix({'loss': loss.item()})\n","\n","\n","    avg_loss = total_loss / len(dataloader)\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    report = classification_report(all_labels, all_preds, target_names=label_map.keys(), zero_division=0)\n","\n","    return avg_loss, accuracy, report, all_labels, all_preds\n"],"metadata":{"id":"reXe2mp4F8AK","executionInfo":{"status":"ok","timestamp":1744004232340,"user_tz":-480,"elapsed":12,"user":{"displayName":"Longfei Ju","userId":"03330376312878383448"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# --- 5. Main Execution ---"],"metadata":{"id":"HLLCEhLWGAOR"}},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    print(f\"Using device: {CONFIG['device']}\")\n","    torch.manual_seed(CONFIG['seed'])\n","\n","    # --- Load Data ---\n","    print(\"Loading labels...\")\n","    try:\n","        df = pd.read_csv(CONFIG['label_file']).dropna(how='all')  #dataframe object\n","        df['ID'] = df['ID'].astype(int)\n","        df['class'] = df['class'].astype(int)\n","        # Basic validation\n","        print(df)\n","        if 'ID' not in df.columns or 'label' not in df.columns:\n","            raise ValueError(\"CSV must contain 'id' and 'label' columns.\")\n","        if not all(label in label_map for label in df['label'].unique()):\n","            raise ValueError(f\"Labels in CSV must be one of {list(label_map.keys())}\")\n","        print(f\"Found {len(df)} samples.\")\n","    except FileNotFoundError:\n","        print(f\"Error: Label file not found at {CONFIG['label_file']}\")\n","        #exit()\n","    except ValueError as e:\n","        print(f\"Error: {e}\")\n","        #exit()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6SkdyZKOK5g4","executionInfo":{"status":"ok","timestamp":1744004236086,"user_tz":-480,"elapsed":1032,"user":{"displayName":"Longfei Ju","userId":"03330376312878383448"}},"outputId":"7178d91e-1e00-43e4-c0a1-b4f2bb57f6a4"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Loading labels...\n","        ID      text     image     label  class\n","0        1   neutral  positive  positive      2\n","1        2   neutral  positive  positive      2\n","2        3   neutral  positive  positive      2\n","3        4  positive  positive  positive      2\n","4        5  positive  positive  positive      2\n","...    ...       ...       ...       ...    ...\n","4506  5125   neutral  positive  positive      2\n","4507  5126  positive   neutral  positive      2\n","4508  5127  positive  positive  positive      2\n","4509  5128   neutral  positive  positive      2\n","4510  5129  positive  positive  positive      2\n","\n","[4511 rows x 5 columns]\n","Found 4511 samples.\n"]}]},{"cell_type":"code","source":["\n","    # --- Split Data ---\n","    # Calculate split sizes\n","    total_size = len(df) # count data rows (exclude title row)\n","    test_size = int(CONFIG['test_split_ratio'] * total_size)  #取0.15的样本用于测试\n","    val_size = int(CONFIG['val_split_ratio'] * total_size) #取0.15的样本用于验证\n","    train_size = total_size - val_size - test_size\n","\n","    print(f\"Splitting data: Train={train_size}, Val={val_size}, Test={test_size}\")\n","    if train_size <= 0 or val_size <= 0 or test_size <= 0:\n","        print(\"Error: Dataset too small for specified split ratios.\")\n","        exit()\n","\n","    # Perform the split   #from torch.utils.data import random_split\n","    #input: 1st : list / dataframe/ tensor /Dataset\n","    #2nd: size #3rd: only for fix seed\n","    train_df, val_df, test_df = random_split(df, [train_size, val_size, test_size],\n","                                             generator=torch.Generator().manual_seed(CONFIG['seed']))\n","\n","    # Convert subsets back to DataFrames for easier indexing if needed by Dataset class\n","    # Note: random_split returns Subset objects. We get the indices and select from the original df.\n","    # drop = true means don't remain the old shuffled indices as a column\n","    train_df = df.iloc[train_df.indices].reset_index(drop=True)\n","    val_df = df.iloc[val_df.indices].reset_index(drop=True)\n","    test_df = df.iloc[test_df.indices].reset_index(drop=True)\n","\n","\n","    # --- Initialize Processor, Dataset, DataLoader ---\n","    print(\"Initializing CLIP Processor...\")\n","    clip_processor = CLIPProcessor.from_pretrained(CONFIG['clip_model_name']) #clip-vit32\n","\n","    print(\"Creating Datasets...\")\n","    #传入df是label的数据，通过此函数去data_dir文件夹取得图片、文本数据，getitem结果：image, text, label\n","    #只是定义了一个取数据的函数，还没实际获得\n","    train_dataset = MultimodalBlogDataset(CONFIG['data_dir'], train_df, clip_processor, label_map,CONFIG['data_dir_img'],CONFIG['data_dir_text'])\n","    val_dataset = MultimodalBlogDataset(CONFIG['data_dir'], val_df, clip_processor, label_map,CONFIG['data_dir_img'],CONFIG['data_dir_text'])\n","    test_dataset = MultimodalBlogDataset(CONFIG['data_dir'], test_df, clip_processor, label_map,CONFIG['data_dir_img'],CONFIG['data_dir_text'])\n","\n","    print(\"Creating DataLoaders...\")\n","    # Define the collate function with necessary arguments partially filled\n","    collate_fn_partial = lambda batch: collate_fn(batch, clip_processor, CONFIG['device'], CONFIG['max_token_length'])\n","    #DataLoader(dataset的子类, bs, shuffle 训练时为true)\n","    #dataloader： 每次从数据源取出一个batch，对元素调用get_item,取完所有元素后，传给collate_fn\n","    #note： dataloader默认只给collate_fn传入batch size，如果要自定义其他参数，需要先写collate_fn_partial\n","    #依然只是构造了dataloader的实例，还不是数据：for inputs, labels in train_loader才是数据\n","    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, collate_fn=collate_fn_partial)\n","    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_fn_partial)\n","    test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_fn_partial)\n","\n","\n","    # --- Initialize Model, Loss, Optimizer ---\n","    print(\"Initializing Model...\")\n","    # Pass ablation flags here\n","    model = MultimodalClassifier(\n","        clip_model_name=CONFIG['clip_model_name'],\n","        num_classes=CONFIG['num_classes'],\n","        use_cross_attention=CONFIG[\"use_cross_attention\"],\n","        use_cnn_layer=CONFIG[\"use_cnn_layer\"],\n","        freeze_clip=CONFIG[\"freeze_clip\"],\n","        processor=clip_processor,\n","        device=CONFIG['device']\n","    )\n","    model = model.to(CONFIG['device']) # Model parts are moved to device in __init__\n","\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # Separate parameters for different learning rates\n","    clip_params = list(model.clip_model.parameters())\n","    head_params = []\n","    if CONFIG[\"use_cross_attention\"]:\n","        head_params.extend(list(model.img_to_txt_attention.parameters()))\n","        head_params.extend(list(model.txt_to_img_attention.parameters()))\n","\n","\n","    optimizer = optim.AdamW([\n","        {'params': clip_params, 'lr': CONFIG['learning_rate_clip']},\n","        {'params': head_params, 'lr': CONFIG['learning_rate_head']}\n","    ])\n","\n","    # --- Training Loop ---\n","    print(\"Starting Training...\")\n","    best_val_accuracy = 0.0\n","    best_epoch = -1\n","\n","    for epoch in range(CONFIG['num_epochs']):\n","        print(f\"\\n--- Epoch {epoch+1}/{CONFIG['num_epochs']} ---\")\n","\n","        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, CONFIG['device'], clip_processor, CONFIG['max_token_length'])\n","        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n","\n","        val_loss, val_acc, val_report, _, _ = evaluate_epoch(model, val_loader, criterion, CONFIG['device'], clip_processor, CONFIG['max_token_length'])\n","        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n","        print(\"Validation Classification Report:\\n\", val_report)\n","\n","        # Save best model based on validation accuracy\n","        if val_acc > best_val_accuracy:\n","            best_val_accuracy = val_acc\n","            best_epoch = epoch\n","            # Create a directory to save models if it doesn't exist\n","            os.makedirs(\"models\", exist_ok=True)\n","            model_save_path = os.path.join(\"models\", \"best_multimodal_model.pth\")\n","            print(f\"Validation accuracy improved. Saving model to {model_save_path}\")\n","            torch.save(model.state_dict(), model_save_path)\n","            # You might want to save optimizer state and epoch number too for resuming training\n","\n","    print(f\"\\nTraining finished. Best validation accuracy ({best_val_accuracy:.4f}) achieved at epoch {best_epoch+1}.\")\n","\n","    # --- Final Evaluation on Test Set ---\n","    print(\"\\n--- Evaluating on Test Set using Best Model ---\")\n","    # Load the best model weights\n","    best_model_path = os.path.join(\"models\", \"best_multimodal_model.pth\")\n","    if os.path.exists(best_model_path):\n","        model.load_state_dict(torch.load(best_model_path, map_location=CONFIG['device']))\n","        print(\"Loaded best model weights for testing.\")\n","\n","        test_loss, test_acc, test_report, test_labels, test_preds = evaluate_epoch(model, test_loader, criterion, CONFIG['device'], clip_processor, CONFIG['max_token_length'])\n","        print(f\"\\nTest Loss: {test_loss:.4f}\")\n","        print(f\"Test Accuracy: {test_acc:.4f}\")\n","        print(\"Test Set Classification Report:\\n\", test_report)\n","        # You can further analyze test_labels and test_preds here (e.g., confusion matrix)\n","    else:\n","        print(\"Warning: Best model file not found. Skipping test set evaluation.\")\n","\n","    print(\"\\nDone.\")\n"],"metadata":{"id":"NWPN39dXGAvR","colab":{"base_uri":"https://localhost:8080/","height":475,"referenced_widgets":["6a3594b79646436c9e13ad498b323e78","4d0141dd46134a529585b5a010d78026","8035bdc1d74d454db52c2b169d0ede3e","e9394a3cb8a547b5926072a9f7488201","94ab9d4a658643bab676e9a60e059bbf","970a5d42c5a6431a99e1d6c59f3d0489","676cbdf1126347ee9af4844190d4843a","5df9408958a3430ca93b6c00c811b89e","b4e5d37110d749719b55c2d7da2fef7e","0a586aab4e28431c88655eae4550ad78","4940f63579304bb0bd6336155c6e402e"]},"outputId":"3d8584fd-4b02-4f19-b029-5bf17b4de8bb","executionInfo":{"status":"error","timestamp":1744007793667,"user_tz":-480,"elapsed":1392600,"user":{"displayName":"Longfei Ju","userId":"03330376312878383448"}}},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Splitting data: Train=3159, Val=676, Test=676\n","Initializing CLIP Processor...\n","Creating Datasets...\n","Creating DataLoaders...\n","Initializing Model...\n","Freezing CLIP model parameters.\n","Starting Training...\n","\n","--- Epoch 1/10 ---\n"]},{"output_type":"display_data","data":{"text/plain":["Training:   0%|          | 0/198 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a3594b79646436c9e13ad498b323e78"}},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-ec8d8cd30026>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n--- Epoch {epoch+1}/{CONFIG['num_epochs']} ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_processor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_token_length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-580979541e80>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, criterion, device, clip_processor, max_length)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprogress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m \u001b[0;31m# Skip if collate_fn returned None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-5b316ac72f6b>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Warning: Text file not found {txt_path}, returning None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}