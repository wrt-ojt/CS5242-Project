{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c95eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "673876e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model\n",
    "class TextRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, num_layers=1, dropout_rate=0.8):\n",
    "        super(TextRNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(self.embedding_dim, self.hidden_size, self.num_layers, batch_first=True, dropout=self.dropout_rate)\n",
    "        \n",
    "        # Dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        # _, hidden = self.rnn(embedded, h0)\n",
    "        out, _ = self.rnn(embedded, h0)\n",
    "\n",
    "        # Apply dropout\n",
    "        # hidden = self.dropout(hidden.squeeze(0))\n",
    "        hidden = self.dropout(out[:, -1, :])\n",
    "        \n",
    "        # Pass the output of the last time step to the classifier\n",
    "        out = self.fc(hidden)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17d66bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset for text files with labels (int[])\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_ids, labels, file_dir, tokenizer, word_to_idx, max_length=20):\n",
    "        self.file_ids = file_ids\n",
    "        self.labels = labels\n",
    "        self.file_dir = file_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_id = self.file_ids[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Read text file\n",
    "        file_path = os.path.join(self.file_dir, f\"{file_id}.txt\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            # print(\"text:\", text)\n",
    "        \n",
    "        # Normalize and Tokenize\n",
    "        tokens = self.tokenizer.tokenize(text.lower())\n",
    "        \n",
    "        # Convert tokens to indices\n",
    "        indices = [self.word_to_idx.get(token, self.word_to_idx['<UNK>']) for token in tokens]\n",
    "        \n",
    "        # Truncate or pad sequence\n",
    "        if len(indices) > self.max_length:\n",
    "            indices = indices[:self.max_length]\n",
    "        else:\n",
    "            indices = indices + [self.word_to_idx['<PAD>']] * (self.max_length - len(indices))\n",
    "            \n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52148b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from all training text files\n",
    "def build_vocabulary(file_paths, tokenizer, min_freq=2, vocab_file='vocabulary_rnn_text_label.txt'):\n",
    "    print(\"Building vocabulary...\")\n",
    "\n",
    "    # load vocabulary from file if it exists\n",
    "    if os.path.exists(vocab_file):\n",
    "        print(f\"Vocabulary file {vocab_file} already exists. Loading...\")\n",
    "        word_to_idx = {}\n",
    "        with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word, idx = line.strip().split('\\t')\n",
    "                word_to_idx[word] = int(idx)\n",
    "        return word_to_idx\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_counts = Counter()\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        tokens = tokenizer.tokenize(text.lower())\n",
    "        word_counts.update(tokens)\n",
    "    \n",
    "    # Filter words by frequency\n",
    "    words = [word for word, count in word_counts.items() if count >= min_freq]\n",
    "    \n",
    "    # Add special tokens\n",
    "    word_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for word in words:\n",
    "        word_to_idx[word] = len(word_to_idx)\n",
    "\n",
    "    # save vocabulary to file\n",
    "    with open(vocab_file, 'w', encoding='utf-8') as f:\n",
    "        for word, idx in word_to_idx.items():\n",
    "            f.write(f\"{word}\\t{idx}\\n\")\n",
    "    print(f\"Vocabulary saved to {vocab_file}\")\n",
    "    \n",
    "    return word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "436bdf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, patience=3, num_epochs=10):\n",
    "    model.to(device)\n",
    "    \n",
    "    best_val_accuracy = 0.0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_predictions = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_predictions.extend(predicted.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_predictions = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(data)\n",
    "                # print(\"outputs:\", outputs, \"labels:\", labels)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                # Get predictions\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_predictions.extend(predicted.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '\n",
    "              f'Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "                break\n",
    "    \n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, best_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ef452f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            # print(\"data:\", data, \"label:\", labels)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            print(\"outputs.data:\", outputs.data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    \n",
    "    print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "    return accuracy, avg_loss, all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "475ab50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_with_splits(model_type='rnn', train=True):\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'val_split_ratio': 0.15,  \n",
    "        'test_split_ratio': 0.15,\n",
    "        'seed': 42,\n",
    "        'batch_size': 16,\n",
    "        'embedding_dim': 100,\n",
    "        'hidden_size': 128,\n",
    "        'num_layers': 2,\n",
    "        'learning_rate': 0.001,\n",
    "        'num_epochs': 10,\n",
    "        'patience': 3\n",
    "    }\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize NLTK tokenizer\n",
    "    tokenizer = TweetTokenizer()\n",
    "    \n",
    "    # Load labels from CSV\n",
    "    labels_df = pd.read_csv('../../label.csv').dropna(how=\"all\")\n",
    "    print(f\"Loaded {len(labels_df)} labels from CSV\")\n",
    "\n",
    "    df = labels_df.copy().dropna(how='all')\n",
    "    df['ID'] = df['ID'].astype(int)\n",
    "    df['class'] = df['class'].astype(int)\n",
    "    \n",
    "    # # Map label text to numerical values\n",
    "    label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "    labels_df['text_numeric_label'] = labels_df['text'].apply(lambda x: label_map.get(x.lower(), 0))\n",
    "    \n",
    "    # # Create a dataframe for splitting\n",
    "    df = labels_df[['ID', 'text_numeric_label', 'label']].rename(columns={'text_numeric_label': 'text_label'})\n",
    "    \n",
    "    # Split the data into train, validation, and test sets\n",
    "    print(\"Splitting data...\")\n",
    "    val_test_size = config['val_split_ratio'] + config['test_split_ratio']\n",
    "    if val_test_size >= 1.0:\n",
    "        print(\"Error: Sum of validation and test split ratios must be less than 1.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Adjust test size relative to the remaining data after validation split\n",
    "    relative_test_size = config['test_split_ratio'] / (1.0 - config['val_split_ratio'])\n",
    "\n",
    "    try:\n",
    "        # Split into train and temp (val + test)\n",
    "        train_df, temp_df = train_test_split(\n",
    "            df,\n",
    "            test_size=val_test_size,\n",
    "            random_state=config['seed'],\n",
    "            stratify=df['text_label'] # Stratify if labels are imbalanced\n",
    "        )\n",
    "        # Split temp into val and test\n",
    "        val_df, test_df = train_test_split(\n",
    "            temp_df,\n",
    "            test_size=relative_test_size,\n",
    "            random_state=config['seed'],\n",
    "            stratify=temp_df['text_label'] # Stratify if labels are imbalanced\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data splitting: {e}. Check split ratios and data.\")\n",
    "        # Might happen if a label class has too few samples for stratification\n",
    "        print(\"Attempting split without stratification...\")\n",
    "        try:\n",
    "            train_df, temp_df = train_test_split(df, test_size=val_test_size, random_state=config['seed'])\n",
    "            val_df, test_df = train_test_split(temp_df, test_size=relative_test_size, random_state=config['seed'])\n",
    "        except Exception as e_nostrat:\n",
    "            print(f\"Error during non-stratified split: {e_nostrat}.\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    print(f\"Train set: {len(train_df)} samples\")\n",
    "    print(f\"Validation set: {len(val_df)} samples\")\n",
    "    print(f\"Test set: {len(test_df)} samples\")\n",
    "    \n",
    "    # Create full file paths for building vocabulary\n",
    "    train_ids = train_df['ID'].astype(int).values\n",
    "    val_ids = val_df['ID'].astype(int).values\n",
    "    test_ids = test_df['ID'].astype(int).values\n",
    "    \n",
    "    train_labels = train_df['text_label'].values\n",
    "    val_labels = val_df['text_label'].values\n",
    "    test_labels = test_df['text_label'].values\n",
    "    \n",
    "    # Build vocabulary from training data only to prevent data leakage\n",
    "    file_paths = [f\"../../raw_data/{id}.txt\" for id in train_ids if os.path.exists(f\"../../raw_data/{id}.txt\")]\n",
    "    word_to_idx = build_vocabulary(file_paths, tokenizer)\n",
    "    vocab_size = len(word_to_idx)\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TextDataset(train_ids, train_labels, '../../raw_data', tokenizer, word_to_idx)\n",
    "    val_dataset = TextDataset(val_ids, val_labels, '../../raw_data', tokenizer, word_to_idx)\n",
    "    test_dataset = TextDataset(test_ids, test_labels, '../../raw_data', tokenizer, word_to_idx)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    \n",
    "    # Initialize the model based on type\n",
    "    num_classes = 3  # negative (0), neutral (1), positive (2)\n",
    "    # if model_type.lower() == 'lstm':\n",
    "    #     model = TextLSTM(vocab_size, config['embedding_dim'], config['hidden_size'], num_classes, config['num_layers'])\n",
    "    #     print(\"Using LSTM model\")\n",
    "    # else:\n",
    "    model = TextRNN(vocab_size, config['embedding_dim'], config['hidden_size'], num_classes, config['num_layers'])\n",
    "    print(\"Using RNN model\")\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    # Train the model if requested\n",
    "    if train:\n",
    "        print(\"Starting training...\")\n",
    "        model, best_val_acc = train_model(\n",
    "            model, \n",
    "            train_loader, \n",
    "            val_loader, \n",
    "            criterion, \n",
    "            optimizer, \n",
    "            device, \n",
    "            patience=config['patience'], \n",
    "            num_epochs=config['num_epochs']\n",
    "        )\n",
    "        \n",
    "        # Save the trained model\n",
    "        model_save_path = f\"{model_type}_text_label_classifier.pth\"\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'vocab': word_to_idx,\n",
    "            'config': config\n",
    "        }, model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "    else:\n",
    "        # Load pre-trained model\n",
    "        model_load_path = f\"{model_type}_text_classifier.pth\"\n",
    "        if os.path.exists(model_load_path):\n",
    "            checkpoint = torch.load(model_load_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(f\"Loaded pre-trained model from {model_load_path}\")\n",
    "        else:\n",
    "            print(f\"No pre-trained model found at {model_load_path}. Using untrained model.\")\n",
    "    \n",
    "    # Evaluate the model on test set\n",
    "    print(\"Evaluating model on test set...\")\n",
    "    test_accuracy, test_loss, test_predictions = evaluate_model(model, test_loader, criterion, device)\n",
    "    \n",
    "    # Create results dataframe for test set\n",
    "    results = pd.DataFrame({\n",
    "        'ID': test_ids,\n",
    "        'true_label': test_labels,\n",
    "        'predicted_label': test_predictions\n",
    "    })\n",
    "    \n",
    "    # Map numeric labels back to text\n",
    "    reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "    results['true_class'] = results['true_label'].map(reverse_label_map)\n",
    "    results['predicted_class'] = results['predicted_label'].map(reverse_label_map)\n",
    "    \n",
    "    # Save results\n",
    "    results.to_csv(f\"{model_type}_text_label_classification_results.csv\", index=False)\n",
    "    print(f\"Results saved to {model_type}_text_label_classification_results.csv\")\n",
    "    \n",
    "    return model, test_accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f0f237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing with RNN model...\n",
      "Using device: cpu\n",
      "Loaded 4511 labels from CSV\n",
      "Splitting data...\n",
      "Train set: 3157 samples\n",
      "Validation set: 1115 samples\n",
      "Test set: 239 samples\n",
      "Building vocabulary...\n",
      "Vocabulary saved to vocabulary_rnn_text_label.txt\n",
      "Vocabulary size: 3589\n",
      "Using RNN model\n",
      "Starting training...\n",
      "Epoch [1/10], Batch [10/198], Loss: 1.1070\n",
      "Epoch [1/10], Batch [20/198], Loss: 1.5191\n",
      "Epoch [1/10], Batch [30/198], Loss: 1.1394\n",
      "Epoch [1/10], Batch [40/198], Loss: 1.3342\n",
      "Epoch [1/10], Batch [50/198], Loss: 1.4804\n",
      "Epoch [1/10], Batch [60/198], Loss: 1.0964\n",
      "Epoch [1/10], Batch [70/198], Loss: 1.3078\n",
      "Epoch [1/10], Batch [80/198], Loss: 1.4676\n",
      "Epoch [1/10], Batch [90/198], Loss: 1.1059\n",
      "Epoch [1/10], Batch [100/198], Loss: 1.1665\n",
      "Epoch [1/10], Batch [110/198], Loss: 1.0077\n",
      "Epoch [1/10], Batch [120/198], Loss: 1.2262\n",
      "Epoch [1/10], Batch [130/198], Loss: 1.1541\n",
      "Epoch [1/10], Batch [140/198], Loss: 1.1361\n",
      "Epoch [1/10], Batch [150/198], Loss: 1.2338\n",
      "Epoch [1/10], Batch [160/198], Loss: 1.1231\n",
      "Epoch [1/10], Batch [170/198], Loss: 1.2757\n",
      "Epoch [1/10], Batch [180/198], Loss: 1.0148\n",
      "Epoch [1/10], Batch [190/198], Loss: 1.0598\n",
      "Epoch [1/10], Train Loss: 1.1717, Train Accuracy: 0.3804, Val Loss: 1.0584, Val Accuracy: 0.4735\n",
      "Epoch [2/10], Batch [10/198], Loss: 0.8819\n",
      "Epoch [2/10], Batch [20/198], Loss: 1.3576\n",
      "Epoch [2/10], Batch [30/198], Loss: 0.9996\n",
      "Epoch [2/10], Batch [40/198], Loss: 0.8346\n",
      "Epoch [2/10], Batch [50/198], Loss: 1.0810\n",
      "Epoch [2/10], Batch [60/198], Loss: 1.3009\n",
      "Epoch [2/10], Batch [70/198], Loss: 1.1565\n",
      "Epoch [2/10], Batch [80/198], Loss: 0.9459\n",
      "Epoch [2/10], Batch [90/198], Loss: 1.0346\n",
      "Epoch [2/10], Batch [100/198], Loss: 1.0900\n",
      "Epoch [2/10], Batch [110/198], Loss: 1.1062\n",
      "Epoch [2/10], Batch [120/198], Loss: 1.1410\n",
      "Epoch [2/10], Batch [130/198], Loss: 0.9554\n",
      "Epoch [2/10], Batch [140/198], Loss: 1.0643\n",
      "Epoch [2/10], Batch [150/198], Loss: 1.0317\n",
      "Epoch [2/10], Batch [160/198], Loss: 1.0329\n",
      "Epoch [2/10], Batch [170/198], Loss: 0.9486\n",
      "Epoch [2/10], Batch [180/198], Loss: 1.0496\n",
      "Epoch [2/10], Batch [190/198], Loss: 1.2339\n",
      "Epoch [2/10], Train Loss: 1.0922, Train Accuracy: 0.4222, Val Loss: 1.0482, Val Accuracy: 0.4834\n",
      "Epoch [3/10], Batch [10/198], Loss: 1.0113\n",
      "Epoch [3/10], Batch [20/198], Loss: 1.1633\n",
      "Epoch [3/10], Batch [30/198], Loss: 0.9744\n",
      "Epoch [3/10], Batch [40/198], Loss: 1.1077\n",
      "Epoch [3/10], Batch [50/198], Loss: 1.0222\n",
      "Epoch [3/10], Batch [60/198], Loss: 1.0358\n",
      "Epoch [3/10], Batch [70/198], Loss: 1.1338\n",
      "Epoch [3/10], Batch [80/198], Loss: 1.1424\n",
      "Epoch [3/10], Batch [90/198], Loss: 1.1079\n",
      "Epoch [3/10], Batch [100/198], Loss: 0.9600\n",
      "Epoch [3/10], Batch [110/198], Loss: 0.9720\n",
      "Epoch [3/10], Batch [120/198], Loss: 1.0463\n",
      "Epoch [3/10], Batch [130/198], Loss: 1.1448\n",
      "Epoch [3/10], Batch [140/198], Loss: 1.0896\n",
      "Epoch [3/10], Batch [150/198], Loss: 0.8376\n",
      "Epoch [3/10], Batch [160/198], Loss: 1.2045\n",
      "Epoch [3/10], Batch [170/198], Loss: 1.1480\n",
      "Epoch [3/10], Batch [180/198], Loss: 1.0315\n",
      "Epoch [3/10], Batch [190/198], Loss: 0.9664\n",
      "Epoch [3/10], Train Loss: 1.0777, Train Accuracy: 0.4245, Val Loss: 1.0341, Val Accuracy: 0.4924\n",
      "Epoch [4/10], Batch [10/198], Loss: 0.9762\n",
      "Epoch [4/10], Batch [20/198], Loss: 1.0572\n",
      "Epoch [4/10], Batch [30/198], Loss: 1.1224\n",
      "Epoch [4/10], Batch [40/198], Loss: 1.0312\n",
      "Epoch [4/10], Batch [50/198], Loss: 0.8695\n",
      "Epoch [4/10], Batch [60/198], Loss: 1.1570\n",
      "Epoch [4/10], Batch [70/198], Loss: 0.9688\n",
      "Epoch [4/10], Batch [80/198], Loss: 1.1888\n",
      "Epoch [4/10], Batch [90/198], Loss: 1.1277\n",
      "Epoch [4/10], Batch [100/198], Loss: 0.9827\n",
      "Epoch [4/10], Batch [110/198], Loss: 0.9392\n",
      "Epoch [4/10], Batch [120/198], Loss: 0.8932\n",
      "Epoch [4/10], Batch [130/198], Loss: 0.9625\n",
      "Epoch [4/10], Batch [140/198], Loss: 1.0217\n",
      "Epoch [4/10], Batch [150/198], Loss: 0.9630\n",
      "Epoch [4/10], Batch [160/198], Loss: 1.2521\n",
      "Epoch [4/10], Batch [170/198], Loss: 0.9738\n",
      "Epoch [4/10], Batch [180/198], Loss: 0.9855\n",
      "Epoch [4/10], Batch [190/198], Loss: 1.1523\n",
      "Epoch [4/10], Train Loss: 1.0555, Train Accuracy: 0.4704, Val Loss: 1.0296, Val Accuracy: 0.5040\n",
      "Epoch [5/10], Batch [10/198], Loss: 0.9784\n",
      "Epoch [5/10], Batch [20/198], Loss: 1.0869\n",
      "Epoch [5/10], Batch [30/198], Loss: 0.8787\n",
      "Epoch [5/10], Batch [40/198], Loss: 0.9985\n",
      "Epoch [5/10], Batch [50/198], Loss: 1.0518\n",
      "Epoch [5/10], Batch [60/198], Loss: 1.2178\n",
      "Epoch [5/10], Batch [70/198], Loss: 1.0574\n",
      "Epoch [5/10], Batch [80/198], Loss: 1.1812\n",
      "Epoch [5/10], Batch [90/198], Loss: 1.1729\n",
      "Epoch [5/10], Batch [100/198], Loss: 0.9649\n",
      "Epoch [5/10], Batch [110/198], Loss: 0.9764\n",
      "Epoch [5/10], Batch [120/198], Loss: 0.9720\n",
      "Epoch [5/10], Batch [130/198], Loss: 0.9422\n",
      "Epoch [5/10], Batch [140/198], Loss: 0.9622\n",
      "Epoch [5/10], Batch [150/198], Loss: 0.9206\n",
      "Epoch [5/10], Batch [160/198], Loss: 1.0863\n",
      "Epoch [5/10], Batch [170/198], Loss: 0.9825\n",
      "Epoch [5/10], Batch [180/198], Loss: 0.8623\n",
      "Epoch [5/10], Batch [190/198], Loss: 1.2545\n",
      "Epoch [5/10], Train Loss: 1.0466, Train Accuracy: 0.4903, Val Loss: 1.0278, Val Accuracy: 0.5085\n",
      "Epoch [6/10], Batch [10/198], Loss: 1.1112\n",
      "Epoch [6/10], Batch [20/198], Loss: 0.9362\n",
      "Epoch [6/10], Batch [30/198], Loss: 1.0542\n",
      "Epoch [6/10], Batch [40/198], Loss: 0.9593\n",
      "Epoch [6/10], Batch [50/198], Loss: 0.8894\n",
      "Epoch [6/10], Batch [60/198], Loss: 1.0968\n",
      "Epoch [6/10], Batch [70/198], Loss: 1.1798\n",
      "Epoch [6/10], Batch [80/198], Loss: 0.7768\n",
      "Epoch [6/10], Batch [90/198], Loss: 1.1397\n",
      "Epoch [6/10], Batch [100/198], Loss: 0.9514\n",
      "Epoch [6/10], Batch [110/198], Loss: 1.1856\n",
      "Epoch [6/10], Batch [120/198], Loss: 1.0389\n",
      "Epoch [6/10], Batch [130/198], Loss: 1.1502\n",
      "Epoch [6/10], Batch [140/198], Loss: 1.1068\n",
      "Epoch [6/10], Batch [150/198], Loss: 0.8397\n",
      "Epoch [6/10], Batch [160/198], Loss: 1.2395\n",
      "Epoch [6/10], Batch [170/198], Loss: 0.7866\n",
      "Epoch [6/10], Batch [180/198], Loss: 0.7782\n",
      "Epoch [6/10], Batch [190/198], Loss: 1.1974\n",
      "Epoch [6/10], Train Loss: 1.0274, Train Accuracy: 0.5125, Val Loss: 1.0363, Val Accuracy: 0.5220\n",
      "Epoch [7/10], Batch [10/198], Loss: 0.9682\n",
      "Epoch [7/10], Batch [20/198], Loss: 0.8326\n",
      "Epoch [7/10], Batch [30/198], Loss: 1.0725\n",
      "Epoch [7/10], Batch [40/198], Loss: 0.9369\n",
      "Epoch [7/10], Batch [50/198], Loss: 0.9149\n",
      "Epoch [7/10], Batch [60/198], Loss: 0.7327\n",
      "Epoch [7/10], Batch [70/198], Loss: 0.9880\n",
      "Epoch [7/10], Batch [80/198], Loss: 0.9447\n",
      "Epoch [7/10], Batch [90/198], Loss: 1.0011\n",
      "Epoch [7/10], Batch [100/198], Loss: 1.0942\n",
      "Epoch [7/10], Batch [110/198], Loss: 1.0048\n",
      "Epoch [7/10], Batch [120/198], Loss: 0.9353\n",
      "Epoch [7/10], Batch [130/198], Loss: 1.2140\n",
      "Epoch [7/10], Batch [140/198], Loss: 0.9819\n",
      "Epoch [7/10], Batch [150/198], Loss: 0.9819\n",
      "Epoch [7/10], Batch [160/198], Loss: 1.0876\n",
      "Epoch [7/10], Batch [170/198], Loss: 1.1266\n",
      "Epoch [7/10], Batch [180/198], Loss: 1.1582\n",
      "Epoch [7/10], Batch [190/198], Loss: 0.9560\n",
      "Epoch [7/10], Train Loss: 1.0285, Train Accuracy: 0.5116, Val Loss: 1.0597, Val Accuracy: 0.4610\n",
      "Epoch [8/10], Batch [10/198], Loss: 1.2170\n",
      "Epoch [8/10], Batch [20/198], Loss: 0.9802\n",
      "Epoch [8/10], Batch [30/198], Loss: 1.1763\n",
      "Epoch [8/10], Batch [40/198], Loss: 1.0485\n",
      "Epoch [8/10], Batch [50/198], Loss: 1.1215\n",
      "Epoch [8/10], Batch [60/198], Loss: 1.1050\n",
      "Epoch [8/10], Batch [70/198], Loss: 1.1702\n",
      "Epoch [8/10], Batch [80/198], Loss: 1.0761\n",
      "Epoch [8/10], Batch [90/198], Loss: 1.0158\n",
      "Epoch [8/10], Batch [100/198], Loss: 1.0456\n",
      "Epoch [8/10], Batch [110/198], Loss: 1.3092\n",
      "Epoch [8/10], Batch [120/198], Loss: 0.9814\n",
      "Epoch [8/10], Batch [130/198], Loss: 0.9837\n",
      "Epoch [8/10], Batch [140/198], Loss: 0.8856\n",
      "Epoch [8/10], Batch [150/198], Loss: 1.0766\n",
      "Epoch [8/10], Batch [160/198], Loss: 1.0977\n",
      "Epoch [8/10], Batch [170/198], Loss: 0.9516\n",
      "Epoch [8/10], Batch [180/198], Loss: 1.0222\n",
      "Epoch [8/10], Batch [190/198], Loss: 1.0803\n",
      "Epoch [8/10], Train Loss: 1.0764, Train Accuracy: 0.4146, Val Loss: 1.0505, Val Accuracy: 0.4430\n",
      "Epoch [9/10], Batch [10/198], Loss: 1.0747\n",
      "Epoch [9/10], Batch [20/198], Loss: 1.1432\n",
      "Epoch [9/10], Batch [30/198], Loss: 1.1834\n",
      "Epoch [9/10], Batch [40/198], Loss: 0.9696\n",
      "Epoch [9/10], Batch [50/198], Loss: 1.0425\n",
      "Epoch [9/10], Batch [60/198], Loss: 0.8838\n",
      "Epoch [9/10], Batch [70/198], Loss: 1.0056\n",
      "Epoch [9/10], Batch [80/198], Loss: 1.0817\n",
      "Epoch [9/10], Batch [90/198], Loss: 1.1608\n",
      "Epoch [9/10], Batch [100/198], Loss: 1.1621\n",
      "Epoch [9/10], Batch [110/198], Loss: 1.1053\n",
      "Epoch [9/10], Batch [120/198], Loss: 1.0188\n",
      "Epoch [9/10], Batch [130/198], Loss: 1.0765\n",
      "Epoch [9/10], Batch [140/198], Loss: 0.9754\n",
      "Epoch [9/10], Batch [150/198], Loss: 1.0543\n",
      "Epoch [9/10], Batch [160/198], Loss: 1.1294\n",
      "Epoch [9/10], Batch [170/198], Loss: 1.0833\n",
      "Epoch [9/10], Batch [180/198], Loss: 1.1275\n",
      "Epoch [9/10], Batch [190/198], Loss: 1.1374\n",
      "Epoch [9/10], Train Loss: 1.0632, Train Accuracy: 0.4381, Val Loss: 1.0169, Val Accuracy: 0.5166\n",
      "Early stopping triggered after 9 epochs\n",
      "Model saved to rnn_text_label_classifier.pth\n",
      "Evaluating model on test set...\n",
      "outputs.data: tensor([[-0.8597,  0.4640, -0.4135],\n",
      "        [-0.8597,  0.4638, -0.4125],\n",
      "        [-0.0453,  0.1869,  0.7937],\n",
      "        [-0.6467,  0.4646,  0.1507],\n",
      "        [-0.8597,  0.4640, -0.4138],\n",
      "        [-0.8595,  0.4352, -0.3123],\n",
      "        [-0.8598,  0.4628, -0.4098],\n",
      "        [-0.8629,  0.4540, -0.3954],\n",
      "        [-0.8610,  0.4352, -0.3183],\n",
      "        [ 0.0016,  0.0770,  0.1444],\n",
      "        [-0.8621,  0.4561, -0.3985],\n",
      "        [ 0.0813,  0.2924,  0.8724],\n",
      "        [-0.8607,  0.4352, -0.3168],\n",
      "        [-0.5502,  0.3720,  0.2933],\n",
      "        [-0.1353,  0.2109,  0.7191],\n",
      "        [-0.8500,  0.4364, -0.2807]])\n",
      "outputs.data: tensor([[ 0.3230, -0.1069,  0.9866],\n",
      "        [-0.8219,  0.4420, -0.2090],\n",
      "        [-0.2998,  0.2956,  0.5791],\n",
      "        [-0.8597,  0.4639, -0.4129],\n",
      "        [ 0.3312,  0.1498,  0.9719],\n",
      "        [-0.8625,  0.4551, -0.3970],\n",
      "        [-0.8507,  0.4363, -0.2830],\n",
      "        [-0.7263,  0.4447, -0.0102],\n",
      "        [-0.4679,  0.3464,  0.4904],\n",
      "        [-0.5677,  0.3949,  0.3277],\n",
      "        [-0.5357,  0.3935,  0.3472],\n",
      "        [-0.6393,  0.4264,  0.1706],\n",
      "        [-0.8597,  0.4640, -0.4138],\n",
      "        [-0.8597,  0.4637, -0.4122],\n",
      "        [-0.5893,  0.4062,  0.2809],\n",
      "        [ 0.2716, -0.1638,  0.9532]])\n",
      "outputs.data: tensor([[ 0.0992,  0.0599,  0.8304],\n",
      "        [-0.2636,  0.2789,  0.6774],\n",
      "        [-0.6145,  0.4028,  0.2425],\n",
      "        [ 0.1084,  0.4253,  0.8633],\n",
      "        [-0.8598,  0.4625, -0.4093],\n",
      "        [-0.3158,  0.3517,  0.6029],\n",
      "        [ 0.0505,  0.2342,  0.8067],\n",
      "        [-0.8597,  0.4633, -0.4111],\n",
      "        [-0.4485,  0.3591,  0.5201],\n",
      "        [-0.7317,  0.4451, -0.0208],\n",
      "        [-0.3003,  0.3313,  0.6819],\n",
      "        [-0.8673,  0.4410, -0.3675],\n",
      "        [-0.8672,  0.4417, -0.3698],\n",
      "        [-0.8606,  0.4600, -0.4045],\n",
      "        [-0.7524,  0.4468, -0.0626],\n",
      "        [-0.3612,  0.3168,  0.4915]])\n",
      "outputs.data: tensor([[-0.8597,  0.4640, -0.4138],\n",
      "        [-0.8661,  0.4366, -0.3450],\n",
      "        [-0.3285,  0.2594,  0.6353],\n",
      "        [-0.6648,  0.4337,  0.1159],\n",
      "        [-0.8660,  0.4461, -0.3813],\n",
      "        [ 0.1400,  0.0640,  0.8560],\n",
      "        [-0.7656,  0.4476, -0.0900],\n",
      "        [-0.7197,  0.4434,  0.0028],\n",
      "        [-0.5323,  0.4245,  0.4292],\n",
      "        [-0.0113,  0.1444,  0.7983],\n",
      "        [-0.7741,  0.4467, -0.1064],\n",
      "        [-0.5969,  0.4020,  0.2711],\n",
      "        [-0.8655,  0.4362, -0.3411],\n",
      "        [-0.6353,  0.4248,  0.1817],\n",
      "        [ 0.1408,  0.1588,  0.8630],\n",
      "        [ 0.2677, -0.2624,  0.8294]])\n",
      "outputs.data: tensor([[ 0.1315,  0.2485,  0.8985],\n",
      "        [-0.0416,  0.2171,  0.7826],\n",
      "        [ 0.3630, -0.1671,  0.8856],\n",
      "        [ 0.2951, -0.0729,  0.9438],\n",
      "        [-0.8579,  0.4353, -0.3064],\n",
      "        [-0.2489,  0.2601,  0.6039],\n",
      "        [-0.8597,  0.4631, -0.4105],\n",
      "        [-0.6735,  0.4356,  0.0963],\n",
      "        [-0.8584,  0.4352, -0.3080],\n",
      "        [ 0.1795,  0.2363,  0.9287],\n",
      "        [-0.8633,  0.4529, -0.3937],\n",
      "        [-0.0333,  0.1157,  0.7205],\n",
      "        [-0.8660,  0.4460, -0.3810],\n",
      "        [-0.8408,  0.4382, -0.2553],\n",
      "        [-0.1579,  0.2615,  0.7778],\n",
      "        [ 0.0425,  0.0089,  0.8756]])\n",
      "outputs.data: tensor([[-0.8665,  0.4446, -0.3779],\n",
      "        [-0.8633,  0.4531, -0.3939],\n",
      "        [-0.1884,  0.1873,  0.5539],\n",
      "        [-0.6983,  0.4408,  0.0456],\n",
      "        [-0.8338,  0.4397, -0.2374],\n",
      "        [-0.5619,  0.1111, -0.2906],\n",
      "        [-0.4873,  0.4190,  0.3786],\n",
      "        [-0.3020,  0.3388,  0.4796],\n",
      "        [-0.6051,  0.4132,  0.2478],\n",
      "        [-0.2436,  0.2553,  0.4824],\n",
      "        [-0.7765,  0.4469, -0.1112],\n",
      "        [-0.8669,  0.4432, -0.3743],\n",
      "        [-0.7697,  0.4465, -0.0977],\n",
      "        [-0.8124,  0.4436, -0.1873],\n",
      "        [-0.4005,  0.3630,  0.5130],\n",
      "        [-0.7624,  0.4471, -0.0826]])\n",
      "outputs.data: tensor([[-0.0797,  0.2282,  0.6988],\n",
      "        [-0.6101,  0.4030,  0.2395],\n",
      "        [-0.1581,  0.2210,  0.6954],\n",
      "        [-0.1864, -0.0034,  0.7370],\n",
      "        [-0.2311,  0.3330,  0.6739],\n",
      "        [-0.8606,  0.4600, -0.4045],\n",
      "        [-0.5881,  0.4081,  0.2857],\n",
      "        [-0.3315,  0.3590,  0.5855],\n",
      "        [-0.4318,  0.3203,  0.4379],\n",
      "        [ 0.2759, -0.1752,  0.9332],\n",
      "        [-0.8597,  0.4640, -0.4133],\n",
      "        [-0.3086,  0.2581,  0.4989],\n",
      "        [-0.7717,  0.4464, -0.1008],\n",
      "        [-0.8541,  0.4357, -0.2934],\n",
      "        [ 0.2816, -0.1282,  0.9657],\n",
      "        [-0.6383,  0.4248,  0.1747]])\n",
      "outputs.data: tensor([[-0.1953,  0.3608,  0.6615],\n",
      "        [-0.6153,  0.4175,  0.2264],\n",
      "        [ 0.3834, -0.0897,  0.8876],\n",
      "        [-0.8383,  0.4388, -0.2487],\n",
      "        [-0.6760,  0.4271,  0.0935],\n",
      "        [-0.8641,  0.4511, -0.3907],\n",
      "        [-0.8600,  0.4618, -0.4077],\n",
      "        [-0.2337,  0.1626,  0.5674],\n",
      "        [-0.8597,  0.4639, -0.4131],\n",
      "        [ 0.1454,  0.1260,  0.8546],\n",
      "        [-0.2052,  0.2565,  0.6620],\n",
      "        [-0.1368,  0.2670,  0.7319],\n",
      "        [-0.0480,  0.0855,  0.6942],\n",
      "        [ 0.0213,  0.1158,  0.9019],\n",
      "        [-0.1668,  0.1301,  0.6156],\n",
      "        [-0.6107,  0.4044,  0.2358]])\n",
      "outputs.data: tensor([[ 0.0681,  0.1220,  0.8706],\n",
      "        [-0.8617,  0.4569, -0.3997],\n",
      "        [-0.8611,  0.4585, -0.4022],\n",
      "        [-0.7202,  0.4436,  0.0022],\n",
      "        [-0.8647,  0.4496, -0.3881],\n",
      "        [ 0.2844, -0.0121,  0.9805],\n",
      "        [-0.0843,  0.1897,  0.7079],\n",
      "        [-0.5417,  0.4260,  0.4067],\n",
      "        [-0.6191,  0.4178,  0.2185],\n",
      "        [-0.0305,  0.1898,  0.7964],\n",
      "        [-0.8598,  0.4628, -0.4098],\n",
      "        [-0.8597,  0.4637, -0.4122],\n",
      "        [-0.5779,  0.4009,  0.3018],\n",
      "        [-0.8315,  0.4404, -0.2322],\n",
      "        [-0.6988,  0.4391,  0.0467],\n",
      "        [-0.8597,  0.4639, -0.4132]])\n",
      "outputs.data: tensor([[ 0.2206,  0.1400,  1.0008],\n",
      "        [-0.8651,  0.4360, -0.3386],\n",
      "        [-0.8602,  0.4611, -0.4064],\n",
      "        [-0.0396,  0.0485,  0.7270],\n",
      "        [-0.8673,  0.4396, -0.3624],\n",
      "        [-0.8597,  0.4640, -0.4135],\n",
      "        [-0.7320,  0.4440, -0.0205],\n",
      "        [-0.8597,  0.4640, -0.4135],\n",
      "        [-0.8609,  0.4589, -0.4029],\n",
      "        [-0.7901,  0.4463, -0.1397],\n",
      "        [-0.6970,  0.4399,  0.0485],\n",
      "        [-0.8488,  0.4366, -0.2773],\n",
      "        [-0.8597,  0.4640, -0.4138],\n",
      "        [-0.8598,  0.4628, -0.4099],\n",
      "        [-0.3419,  0.3557,  0.5620],\n",
      "        [-0.8594,  0.4645, -0.4145]])\n",
      "outputs.data: tensor([[-6.6520e-01,  4.3309e-01, -3.7715e-05],\n",
      "        [-8.5981e-01,  4.6256e-01, -4.0932e-01],\n",
      "        [-8.5980e-01,  4.6260e-01, -4.0940e-01],\n",
      "        [-2.1149e-01,  2.9314e-01,  7.6334e-01],\n",
      "        [-6.4787e-01,  4.2697e-01,  1.5497e-01],\n",
      "        [-7.9164e-01,  4.4593e-01, -1.4246e-01],\n",
      "        [-6.6875e-01,  4.3417e-01,  1.0698e-01],\n",
      "        [-5.6502e-01,  3.9603e-01,  3.2566e-01],\n",
      "        [-1.2024e-01,  1.3594e-01,  7.2946e-01],\n",
      "        [-6.8640e-01,  4.3036e-01,  7.1865e-02],\n",
      "        [-4.2073e-01,  3.6371e-01,  5.3220e-01],\n",
      "        [-7.8099e-01,  4.4667e-01, -1.2040e-01],\n",
      "        [-8.5970e-01,  4.6403e-01, -4.1376e-01],\n",
      "        [-8.1899e-01,  4.4241e-01, -2.0216e-01],\n",
      "        [-7.4763e-01,  4.4592e-01, -5.2242e-02],\n",
      "        [-3.7581e-01,  3.5566e-01,  5.3342e-01]])\n",
      "outputs.data: tensor([[-0.8610,  0.4587, -0.4025],\n",
      "        [ 0.4137, -0.0632,  1.0120],\n",
      "        [-0.5271,  0.3978,  0.4382],\n",
      "        [ 0.0332,  0.0798,  0.7884],\n",
      "        [ 0.3108, -0.1471,  0.9700],\n",
      "        [ 0.2505,  0.0783,  0.9300],\n",
      "        [-0.8654,  0.4479, -0.3849],\n",
      "        [ 0.0520,  0.2060,  0.8380],\n",
      "        [-0.6665,  0.4335,  0.1128],\n",
      "        [-0.8645,  0.4501, -0.3890],\n",
      "        [-0.0903,  0.1639,  0.7048],\n",
      "        [-0.0855,  0.4582,  0.7937],\n",
      "        [-0.4109,  0.3635,  0.5247],\n",
      "        [ 0.0106,  0.2660,  0.8017],\n",
      "        [ 0.0166,  0.2629,  0.7888],\n",
      "        [-0.0648,  0.1250,  0.7146]])\n",
      "outputs.data: tensor([[-0.6606,  0.4178,  0.1303],\n",
      "        [-0.6970,  0.4398,  0.0499],\n",
      "        [-0.8155,  0.4430, -0.1942],\n",
      "        [-0.5931,  0.3974,  0.2737],\n",
      "        [-0.8530,  0.4359, -0.2899],\n",
      "        [-0.8622,  0.4352, -0.3230],\n",
      "        [-0.8597,  0.4640, -0.4138],\n",
      "        [-0.5847,  0.3928,  0.2994],\n",
      "        [-0.8667,  0.4439, -0.3762],\n",
      "        [-0.6159,  0.2913, -0.0220],\n",
      "        [-0.3676,  0.3443,  0.5862],\n",
      "        [-0.7552,  0.4468, -0.0682],\n",
      "        [-0.1738,  0.2909,  0.7331],\n",
      "        [ 0.1671,  0.1675,  0.8743],\n",
      "        [-0.1556,  0.2043,  0.7258],\n",
      "        [ 0.1944,  0.0495,  0.9038]])\n",
      "outputs.data: tensor([[-0.7321,  0.4211,  0.0290],\n",
      "        [-0.8637,  0.4355, -0.3307],\n",
      "        [-0.8616,  0.4352, -0.3208],\n",
      "        [-0.5102,  0.3967,  0.4508],\n",
      "        [-0.0323,  0.0825,  0.7530],\n",
      "        [-0.7389,  0.4459, -0.0357],\n",
      "        [-0.1797,  0.1752,  0.6482],\n",
      "        [-0.8600,  0.4620, -0.4081],\n",
      "        [-0.8666,  0.4372, -0.3496],\n",
      "        [-0.8661,  0.4459, -0.3809],\n",
      "        [-0.1816,  0.3412,  0.6866],\n",
      "        [ 0.4187,  0.1348,  1.0507],\n",
      "        [-0.6941,  0.4399,  0.0542],\n",
      "        [-0.1261,  0.2611,  0.7209],\n",
      "        [-0.8272,  0.4410, -0.2212],\n",
      "        [-0.1250,  0.2790,  0.7091]])\n",
      "outputs.data: tensor([[-0.8597,  0.4639, -0.4131],\n",
      "        [-0.8597,  0.4638, -0.4127],\n",
      "        [-0.2518,  0.2735,  0.6100],\n",
      "        [-0.8660,  0.4366, -0.3447],\n",
      "        [ 0.3631,  0.0501,  1.0145],\n",
      "        [-0.2756,  0.3702,  0.6325],\n",
      "        [-0.2678,  0.1833,  0.6722],\n",
      "        [-0.5245,  0.4001,  0.4610],\n",
      "        [-0.7706,  0.4468, -0.0985],\n",
      "        [-0.8632,  0.4532, -0.3941],\n",
      "        [ 0.3235,  0.1555,  1.0402],\n",
      "        [-0.8622,  0.4352, -0.3235],\n",
      "        [-0.0541,  0.0937,  0.7421],\n",
      "        [ 0.2867, -0.1679,  0.9109],\n",
      "        [-0.3764,  0.3711,  0.4161]])\n",
      "Test Loss: 1.0496, Test Accuracy: 0.4393\n",
      "Results saved to rnn_text_label_classification_results.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing with RNN model...\")\n",
    "rnn_model, rnn_accuracy, rnn_loss = process_data_with_splits(model_type='rnn_text_label_with_dropout', train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca9f16b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of models:\n",
      "RNN - Accuracy: 0.4393, Loss: 1.0496\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComparison of models:\")\n",
    "print(f\"RNN - Accuracy: {rnn_accuracy:.4f}, Loss: {rnn_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23750444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5242-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
