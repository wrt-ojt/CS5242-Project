{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c95eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "673876e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model for classification\n",
    "class TextLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes, num_layers=1, dropout_rate=0.8):\n",
    "        super(TextLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, dropout=dropout_rate)\n",
    "        \n",
    "        # Dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # Initialize hidden state and cell state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(embedded, (h0, c0))\n",
    "\n",
    "        # Apply dropout\n",
    "        out = self.dropout(out[:, -1, :])  # Get the last time step output\n",
    "        \n",
    "        # Pass the output of the last time step to the classifier\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17d66bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset for text files with labels\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_ids, labels, file_dir, tokenizer, word_to_idx, max_length=20):\n",
    "        self.file_ids = file_ids\n",
    "        self.labels = labels\n",
    "        self.file_dir = file_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_id = self.file_ids[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Read text file\n",
    "        file_path = os.path.join(self.file_dir, f\"{file_id}.txt\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        # Normalize and Tokenize\n",
    "        tokens = self.tokenizer.tokenize(text.lower())\n",
    "        \n",
    "        # Convert tokens to indices\n",
    "        indices = [self.word_to_idx.get(token, self.word_to_idx['<UNK>']) for token in tokens]\n",
    "        \n",
    "        # Truncate or pad sequence\n",
    "        if len(indices) > self.max_length:\n",
    "            indices = indices[:self.max_length]\n",
    "        else:\n",
    "            indices = indices + [self.word_to_idx['<PAD>']] * (self.max_length - len(indices))\n",
    "            \n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d52148b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from all text files\n",
    "def build_vocabulary(file_paths, tokenizer, min_freq=2, vocab_file='vocabulary_lstm_text_label.txt'):\n",
    "    print(\"Building vocabulary...\")\n",
    "\n",
    "    # load vocabulary from file if it exists\n",
    "    if os.path.exists(vocab_file):\n",
    "        print(f\"Vocabulary file {vocab_file} already exists. Loading...\")\n",
    "        word_to_idx = {}\n",
    "        with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word, idx = line.strip().split('\\t')\n",
    "                word_to_idx[word] = int(idx)\n",
    "        return word_to_idx\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_counts = Counter()\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        tokens = tokenizer.tokenize(text.lower())\n",
    "        word_counts.update(tokens)\n",
    "    \n",
    "    # Filter words by frequency\n",
    "    words = [word for word, count in word_counts.items() if count >= min_freq]\n",
    "    \n",
    "    # Add special tokens\n",
    "    word_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for word in words:\n",
    "        word_to_idx[word] = len(word_to_idx)\n",
    "\n",
    "    # save vocabulary to file\n",
    "    with open(vocab_file, 'w', encoding='utf-8') as f:\n",
    "        for word, idx in word_to_idx.items():\n",
    "            f.write(f\"{word}\\t{idx}\\n\")\n",
    "    print(f\"Vocabulary saved to {vocab_file}\")\n",
    "    \n",
    "    return word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "436bdf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, patience=3, num_epochs=10):\n",
    "    model.to(device)\n",
    "    \n",
    "    best_val_accuracy = 0.0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_predictions = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_predictions.extend(predicted.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_predictions = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                # Get predictions\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_predictions.extend(predicted.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '\n",
    "              f'Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "                break\n",
    "    \n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, best_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ef452f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    \n",
    "    print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "    return accuracy, avg_loss, all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "475ab50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_with_splits(model_type='lstm_text_label_with_dropout', train=True):\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'val_split_ratio': 0.15,  \n",
    "        'test_split_ratio': 0.15,\n",
    "        'seed': 42,\n",
    "        'batch_size': 16,\n",
    "        'embedding_dim': 100,\n",
    "        'hidden_size': 128,\n",
    "        'num_layers': 2,\n",
    "        'learning_rate': 0.001,\n",
    "        'num_epochs': 15,\n",
    "        'patience': 3\n",
    "    }\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize NLTK tokenizer\n",
    "    tokenizer = TweetTokenizer()\n",
    "    \n",
    "    # Load labels from CSV\n",
    "    labels_df = pd.read_csv('../../label.csv').dropna(how='all')\n",
    "    print(f\"Loaded {len(labels_df)} labels from CSV\")\n",
    "\n",
    "    df = labels_df.copy().dropna(how='all')\n",
    "    df['ID'] = df['ID'].astype(int)\n",
    "    df['class'] = df['class'].astype(int)\n",
    "    \n",
    "    # # Map label text to numerical values\n",
    "    label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "    labels_df['text_numeric_label'] = labels_df['text'].apply(lambda x: label_map.get(x.lower(), 0))\n",
    "    \n",
    "    # # Create a dataframe for splitting\n",
    "    df = labels_df[['ID', 'text_numeric_label', 'label']].rename(columns={'text_numeric_label': 'text_label'})\n",
    "    \n",
    "    # Split the data into train, validation, and test sets\n",
    "    print(\"Splitting data...\")\n",
    "    val_test_size = config['val_split_ratio'] + config['test_split_ratio']\n",
    "    if val_test_size >= 1.0:\n",
    "        print(\"Error: Sum of validation and test split ratios must be less than 1.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Adjust test size relative to the remaining data after validation split\n",
    "    relative_test_size = config['test_split_ratio'] / (1.0 - config['val_split_ratio'])\n",
    "\n",
    "    try:\n",
    "        # Split into train and temp (val + test)\n",
    "        train_df, temp_df = train_test_split(\n",
    "            df,\n",
    "            test_size=val_test_size,\n",
    "            random_state=config['seed'],\n",
    "            stratify=df['text_label'] # Stratify if labels are imbalanced\n",
    "        )\n",
    "        # Split temp into val and test\n",
    "        val_df, test_df = train_test_split(\n",
    "            temp_df,\n",
    "            test_size=relative_test_size,\n",
    "            random_state=config['seed'],\n",
    "            stratify=temp_df['text_label'] # Stratify if labels are imbalanced\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data splitting: {e}. Check split ratios and data.\")\n",
    "        # Might happen if a label class has too few samples for stratification\n",
    "        print(\"Attempting split without stratification...\")\n",
    "        try:\n",
    "            train_df, temp_df = train_test_split(df, test_size=val_test_size, random_state=config['seed'])\n",
    "            val_df, test_df = train_test_split(temp_df, test_size=relative_test_size, random_state=config['seed'])\n",
    "        except Exception as e_nostrat:\n",
    "            print(f\"Error during non-stratified split: {e_nostrat}.\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    print(f\"Train set: {len(train_df)} samples\")\n",
    "    print(f\"Validation set: {len(val_df)} samples\")\n",
    "    print(f\"Test set: {len(test_df)} samples\")\n",
    "    \n",
    "    # Create full file paths for building vocabulary\n",
    "    train_ids = train_df['ID'].astype(int).values\n",
    "    val_ids = val_df['ID'].astype(int).values\n",
    "    test_ids = test_df['ID'].astype(int).values\n",
    "    \n",
    "    train_labels = train_df['text_label'].values\n",
    "    val_labels = val_df['text_label'].values\n",
    "    test_labels = test_df['text_label'].values\n",
    "    \n",
    "    # Build vocabulary from training data only to prevent data leakage\n",
    "    file_paths = [f\"../../raw_data/{id}.txt\" for id in train_ids if os.path.exists(f\"../../raw_data/{id}.txt\")]\n",
    "    word_to_idx = build_vocabulary(file_paths, tokenizer)\n",
    "    vocab_size = len(word_to_idx)\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TextDataset(train_ids, train_labels, '../../raw_data', tokenizer, word_to_idx)\n",
    "    val_dataset = TextDataset(val_ids, val_labels, '../../raw_data', tokenizer, word_to_idx)\n",
    "    test_dataset = TextDataset(test_ids, test_labels, '../../raw_data', tokenizer, word_to_idx)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    \n",
    "    # Initialize the model based on type\n",
    "    num_classes = 3  # negative (0), neutral (1), positive (2)\n",
    "    # if model_type.lower() == 'lstm':\n",
    "    model = TextLSTM(vocab_size, config['embedding_dim'], config['hidden_size'], num_classes, config['num_layers'])\n",
    "    print(\"Using LSTM model\")\n",
    "    # else:\n",
    "    #     model = TextRNN(vocab_size, config['embedding_dim'], config['hidden_size'], num_classes, config['num_layers'])\n",
    "    #     print(\"Using RNN model\")\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    # Train the model if requested\n",
    "    if train:\n",
    "        print(\"Starting training...\")\n",
    "        model, best_val_acc = train_model(\n",
    "            model, \n",
    "            train_loader, \n",
    "            val_loader, \n",
    "            criterion, \n",
    "            optimizer, \n",
    "            device, \n",
    "            patience=config['patience'], \n",
    "            num_epochs=config['num_epochs']\n",
    "        )\n",
    "        \n",
    "        # Save the trained model\n",
    "        model_save_path = f\"{model_type}_text_classifier.pth\"\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'vocab': word_to_idx,\n",
    "            'config': config\n",
    "        }, model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "    else:\n",
    "        # Load pre-trained model\n",
    "        model_load_path = f\"{model_type}_text_classifier.pth\"\n",
    "        if os.path.exists(model_load_path):\n",
    "            checkpoint = torch.load(model_load_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(f\"Loaded pre-trained model from {model_load_path}\")\n",
    "        else:\n",
    "            print(f\"No pre-trained model found at {model_load_path}. Using untrained model.\")\n",
    "    \n",
    "    # Evaluate the model on test set\n",
    "    print(\"Evaluating model on test set...\")\n",
    "    test_accuracy, test_loss, test_predictions = evaluate_model(model, test_loader, criterion, device)\n",
    "    \n",
    "    # Create results dataframe for test set\n",
    "    results = pd.DataFrame({\n",
    "        'ID': test_ids,\n",
    "        'true_label': test_labels,\n",
    "        'predicted_label': test_predictions\n",
    "    })\n",
    "    \n",
    "    # Map numeric labels back to text\n",
    "    reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "    results['true_class'] = results['true_label'].map(reverse_label_map)\n",
    "    results['predicted_class'] = results['predicted_label'].map(reverse_label_map)\n",
    "    \n",
    "    # Save results\n",
    "    results.to_csv(f\"{model_type}_classification_results.csv\", index=False)\n",
    "    print(f\"Results saved to {model_type}_classification_results.csv\")\n",
    "    \n",
    "    return model, test_accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37f0f237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing with LSTM model...\n",
      "Using device: cpu\n",
      "Loaded 4511 labels from CSV\n",
      "Splitting data...\n",
      "Train set: 3157 samples\n",
      "Validation set: 1115 samples\n",
      "Test set: 239 samples\n",
      "Building vocabulary...\n",
      "Vocabulary saved to vocabulary_lstm_text_label.txt\n",
      "Vocabulary size: 3589\n",
      "Using LSTM model\n",
      "Starting training...\n",
      "Epoch [1/15], Batch [10/198], Loss: 1.0957\n",
      "Epoch [1/15], Batch [20/198], Loss: 1.1090\n",
      "Epoch [1/15], Batch [30/198], Loss: 1.0262\n",
      "Epoch [1/15], Batch [40/198], Loss: 1.0269\n",
      "Epoch [1/15], Batch [50/198], Loss: 1.0956\n",
      "Epoch [1/15], Batch [60/198], Loss: 1.0397\n",
      "Epoch [1/15], Batch [70/198], Loss: 0.9271\n",
      "Epoch [1/15], Batch [80/198], Loss: 1.1575\n",
      "Epoch [1/15], Batch [90/198], Loss: 1.0776\n",
      "Epoch [1/15], Batch [100/198], Loss: 1.0643\n",
      "Epoch [1/15], Batch [110/198], Loss: 1.1435\n",
      "Epoch [1/15], Batch [120/198], Loss: 0.9509\n",
      "Epoch [1/15], Batch [130/198], Loss: 0.9494\n",
      "Epoch [1/15], Batch [140/198], Loss: 0.9623\n",
      "Epoch [1/15], Batch [150/198], Loss: 1.0474\n",
      "Epoch [1/15], Batch [160/198], Loss: 0.9994\n",
      "Epoch [1/15], Batch [170/198], Loss: 1.0799\n",
      "Epoch [1/15], Batch [180/198], Loss: 0.9838\n",
      "Epoch [1/15], Batch [190/198], Loss: 0.9615\n",
      "Epoch [1/15], Train Loss: 1.0577, Train Accuracy: 0.4564, Val Loss: 1.0916, Val Accuracy: 0.4323\n",
      "Epoch [2/15], Batch [10/198], Loss: 0.9257\n",
      "Epoch [2/15], Batch [20/198], Loss: 1.0889\n",
      "Epoch [2/15], Batch [30/198], Loss: 1.0499\n",
      "Epoch [2/15], Batch [40/198], Loss: 1.0418\n",
      "Epoch [2/15], Batch [50/198], Loss: 1.0460\n",
      "Epoch [2/15], Batch [60/198], Loss: 1.1790\n",
      "Epoch [2/15], Batch [70/198], Loss: 1.0200\n",
      "Epoch [2/15], Batch [80/198], Loss: 1.0312\n",
      "Epoch [2/15], Batch [90/198], Loss: 1.1411\n",
      "Epoch [2/15], Batch [100/198], Loss: 0.8575\n",
      "Epoch [2/15], Batch [110/198], Loss: 1.0588\n",
      "Epoch [2/15], Batch [120/198], Loss: 1.0462\n",
      "Epoch [2/15], Batch [130/198], Loss: 1.0110\n",
      "Epoch [2/15], Batch [140/198], Loss: 0.9901\n",
      "Epoch [2/15], Batch [150/198], Loss: 1.1984\n",
      "Epoch [2/15], Batch [160/198], Loss: 0.8305\n",
      "Epoch [2/15], Batch [170/198], Loss: 1.0413\n",
      "Epoch [2/15], Batch [180/198], Loss: 0.9091\n",
      "Epoch [2/15], Batch [190/198], Loss: 0.9970\n",
      "Epoch [2/15], Train Loss: 1.0108, Train Accuracy: 0.5138, Val Loss: 0.9686, Val Accuracy: 0.5399\n",
      "Epoch [3/15], Batch [10/198], Loss: 0.6762\n",
      "Epoch [3/15], Batch [20/198], Loss: 0.8790\n",
      "Epoch [3/15], Batch [30/198], Loss: 0.9110\n",
      "Epoch [3/15], Batch [40/198], Loss: 0.9476\n",
      "Epoch [3/15], Batch [50/198], Loss: 1.1882\n",
      "Epoch [3/15], Batch [60/198], Loss: 1.0867\n",
      "Epoch [3/15], Batch [70/198], Loss: 0.8219\n",
      "Epoch [3/15], Batch [80/198], Loss: 0.9927\n",
      "Epoch [3/15], Batch [90/198], Loss: 1.2026\n",
      "Epoch [3/15], Batch [100/198], Loss: 0.8009\n",
      "Epoch [3/15], Batch [110/198], Loss: 0.9533\n",
      "Epoch [3/15], Batch [120/198], Loss: 0.6991\n",
      "Epoch [3/15], Batch [130/198], Loss: 0.9925\n",
      "Epoch [3/15], Batch [140/198], Loss: 0.6122\n",
      "Epoch [3/15], Batch [150/198], Loss: 1.2022\n",
      "Epoch [3/15], Batch [160/198], Loss: 0.9338\n",
      "Epoch [3/15], Batch [170/198], Loss: 0.9887\n",
      "Epoch [3/15], Batch [180/198], Loss: 0.9085\n",
      "Epoch [3/15], Batch [190/198], Loss: 0.7689\n",
      "Epoch [3/15], Train Loss: 0.9330, Train Accuracy: 0.5781, Val Loss: 0.9307, Val Accuracy: 0.5668\n",
      "Epoch [4/15], Batch [10/198], Loss: 0.8025\n",
      "Epoch [4/15], Batch [20/198], Loss: 0.8985\n",
      "Epoch [4/15], Batch [30/198], Loss: 0.7279\n",
      "Epoch [4/15], Batch [40/198], Loss: 0.5789\n",
      "Epoch [4/15], Batch [50/198], Loss: 0.8617\n",
      "Epoch [4/15], Batch [60/198], Loss: 0.8238\n",
      "Epoch [4/15], Batch [70/198], Loss: 0.7368\n",
      "Epoch [4/15], Batch [80/198], Loss: 0.5797\n",
      "Epoch [4/15], Batch [90/198], Loss: 0.7433\n",
      "Epoch [4/15], Batch [100/198], Loss: 0.9681\n",
      "Epoch [4/15], Batch [110/198], Loss: 0.7762\n",
      "Epoch [4/15], Batch [120/198], Loss: 1.5964\n",
      "Epoch [4/15], Batch [130/198], Loss: 0.7242\n",
      "Epoch [4/15], Batch [140/198], Loss: 0.9966\n",
      "Epoch [4/15], Batch [150/198], Loss: 0.9747\n",
      "Epoch [4/15], Batch [160/198], Loss: 0.8903\n",
      "Epoch [4/15], Batch [170/198], Loss: 1.1237\n",
      "Epoch [4/15], Batch [180/198], Loss: 0.9579\n",
      "Epoch [4/15], Batch [190/198], Loss: 0.7733\n",
      "Epoch [4/15], Train Loss: 0.8292, Train Accuracy: 0.6383, Val Loss: 0.9899, Val Accuracy: 0.5668\n",
      "Epoch [5/15], Batch [10/198], Loss: 0.8819\n",
      "Epoch [5/15], Batch [20/198], Loss: 0.5160\n",
      "Epoch [5/15], Batch [30/198], Loss: 0.5130\n",
      "Epoch [5/15], Batch [40/198], Loss: 0.5281\n",
      "Epoch [5/15], Batch [50/198], Loss: 0.5322\n",
      "Epoch [5/15], Batch [60/198], Loss: 1.0156\n",
      "Epoch [5/15], Batch [70/198], Loss: 0.6173\n",
      "Epoch [5/15], Batch [80/198], Loss: 0.9116\n",
      "Epoch [5/15], Batch [90/198], Loss: 0.6230\n",
      "Epoch [5/15], Batch [100/198], Loss: 0.6767\n",
      "Epoch [5/15], Batch [110/198], Loss: 0.8751\n",
      "Epoch [5/15], Batch [120/198], Loss: 1.0622\n",
      "Epoch [5/15], Batch [130/198], Loss: 0.4210\n",
      "Epoch [5/15], Batch [140/198], Loss: 0.8951\n",
      "Epoch [5/15], Batch [150/198], Loss: 0.8572\n",
      "Epoch [5/15], Batch [160/198], Loss: 0.8573\n",
      "Epoch [5/15], Batch [170/198], Loss: 1.1039\n",
      "Epoch [5/15], Batch [180/198], Loss: 0.9389\n",
      "Epoch [5/15], Batch [190/198], Loss: 0.3402\n",
      "Epoch [5/15], Train Loss: 0.7030, Train Accuracy: 0.7127, Val Loss: 0.9712, Val Accuracy: 0.5749\n",
      "Epoch [6/15], Batch [10/198], Loss: 0.3991\n",
      "Epoch [6/15], Batch [20/198], Loss: 0.2728\n",
      "Epoch [6/15], Batch [30/198], Loss: 0.3311\n",
      "Epoch [6/15], Batch [40/198], Loss: 0.9639\n",
      "Epoch [6/15], Batch [50/198], Loss: 0.2414\n",
      "Epoch [6/15], Batch [60/198], Loss: 0.5117\n",
      "Epoch [6/15], Batch [70/198], Loss: 1.0807\n",
      "Epoch [6/15], Batch [80/198], Loss: 0.6558\n",
      "Epoch [6/15], Batch [90/198], Loss: 0.5389\n",
      "Epoch [6/15], Batch [100/198], Loss: 0.4830\n",
      "Epoch [6/15], Batch [110/198], Loss: 0.4553\n",
      "Epoch [6/15], Batch [120/198], Loss: 0.2830\n",
      "Epoch [6/15], Batch [130/198], Loss: 0.7852\n",
      "Epoch [6/15], Batch [140/198], Loss: 0.3177\n",
      "Epoch [6/15], Batch [150/198], Loss: 0.9606\n",
      "Epoch [6/15], Batch [160/198], Loss: 0.5095\n",
      "Epoch [6/15], Batch [170/198], Loss: 0.8662\n",
      "Epoch [6/15], Batch [180/198], Loss: 0.6633\n",
      "Epoch [6/15], Batch [190/198], Loss: 0.5358\n",
      "Epoch [6/15], Train Loss: 0.5907, Train Accuracy: 0.7754, Val Loss: 1.0264, Val Accuracy: 0.5883\n",
      "Epoch [7/15], Batch [10/198], Loss: 0.3899\n",
      "Epoch [7/15], Batch [20/198], Loss: 0.5819\n",
      "Epoch [7/15], Batch [30/198], Loss: 0.3621\n",
      "Epoch [7/15], Batch [40/198], Loss: 0.4696\n",
      "Epoch [7/15], Batch [50/198], Loss: 0.2862\n",
      "Epoch [7/15], Batch [60/198], Loss: 0.3358\n",
      "Epoch [7/15], Batch [70/198], Loss: 0.5976\n",
      "Epoch [7/15], Batch [80/198], Loss: 0.4122\n",
      "Epoch [7/15], Batch [90/198], Loss: 0.5140\n",
      "Epoch [7/15], Batch [100/198], Loss: 0.4220\n",
      "Epoch [7/15], Batch [110/198], Loss: 0.6179\n",
      "Epoch [7/15], Batch [120/198], Loss: 0.5454\n",
      "Epoch [7/15], Batch [130/198], Loss: 0.7873\n",
      "Epoch [7/15], Batch [140/198], Loss: 0.3189\n",
      "Epoch [7/15], Batch [150/198], Loss: 0.5089\n",
      "Epoch [7/15], Batch [160/198], Loss: 0.4174\n",
      "Epoch [7/15], Batch [170/198], Loss: 0.2707\n",
      "Epoch [7/15], Batch [180/198], Loss: 0.2231\n",
      "Epoch [7/15], Batch [190/198], Loss: 0.3879\n",
      "Epoch [7/15], Train Loss: 0.4805, Train Accuracy: 0.8226, Val Loss: 1.1233, Val Accuracy: 0.5865\n",
      "Epoch [8/15], Batch [10/198], Loss: 0.2850\n",
      "Epoch [8/15], Batch [20/198], Loss: 0.3740\n",
      "Epoch [8/15], Batch [30/198], Loss: 0.3522\n",
      "Epoch [8/15], Batch [40/198], Loss: 0.7961\n",
      "Epoch [8/15], Batch [50/198], Loss: 0.3033\n",
      "Epoch [8/15], Batch [60/198], Loss: 0.4349\n",
      "Epoch [8/15], Batch [70/198], Loss: 0.9773\n",
      "Epoch [8/15], Batch [80/198], Loss: 0.7242\n",
      "Epoch [8/15], Batch [90/198], Loss: 0.6247\n",
      "Epoch [8/15], Batch [100/198], Loss: 0.2964\n",
      "Epoch [8/15], Batch [110/198], Loss: 0.1890\n",
      "Epoch [8/15], Batch [120/198], Loss: 0.8712\n",
      "Epoch [8/15], Batch [130/198], Loss: 0.4945\n",
      "Epoch [8/15], Batch [140/198], Loss: 0.8781\n",
      "Epoch [8/15], Batch [150/198], Loss: 0.2990\n",
      "Epoch [8/15], Batch [160/198], Loss: 0.2956\n",
      "Epoch [8/15], Batch [170/198], Loss: 0.3665\n",
      "Epoch [8/15], Batch [180/198], Loss: 0.0804\n",
      "Epoch [8/15], Batch [190/198], Loss: 0.1660\n",
      "Epoch [8/15], Train Loss: 0.3800, Train Accuracy: 0.8695, Val Loss: 1.2446, Val Accuracy: 0.6099\n",
      "Epoch [9/15], Batch [10/198], Loss: 0.3186\n",
      "Epoch [9/15], Batch [20/198], Loss: 0.3468\n",
      "Epoch [9/15], Batch [30/198], Loss: 0.6682\n",
      "Epoch [9/15], Batch [40/198], Loss: 0.2536\n",
      "Epoch [9/15], Batch [50/198], Loss: 0.4770\n",
      "Epoch [9/15], Batch [60/198], Loss: 0.1563\n",
      "Epoch [9/15], Batch [70/198], Loss: 0.5162\n",
      "Epoch [9/15], Batch [80/198], Loss: 0.1785\n",
      "Epoch [9/15], Batch [90/198], Loss: 0.3858\n",
      "Epoch [9/15], Batch [100/198], Loss: 0.4904\n",
      "Epoch [9/15], Batch [110/198], Loss: 0.4133\n",
      "Epoch [9/15], Batch [120/198], Loss: 0.3756\n",
      "Epoch [9/15], Batch [130/198], Loss: 0.2168\n",
      "Epoch [9/15], Batch [140/198], Loss: 0.4519\n",
      "Epoch [9/15], Batch [150/198], Loss: 0.1679\n",
      "Epoch [9/15], Batch [160/198], Loss: 0.1997\n",
      "Epoch [9/15], Batch [170/198], Loss: 0.2816\n",
      "Epoch [9/15], Batch [180/198], Loss: 0.0969\n",
      "Epoch [9/15], Batch [190/198], Loss: 0.2085\n",
      "Epoch [9/15], Train Loss: 0.3002, Train Accuracy: 0.9113, Val Loss: 1.4743, Val Accuracy: 0.5991\n",
      "Epoch [10/15], Batch [10/198], Loss: 0.1365\n",
      "Epoch [10/15], Batch [20/198], Loss: 0.5492\n",
      "Epoch [10/15], Batch [30/198], Loss: 0.4370\n",
      "Epoch [10/15], Batch [40/198], Loss: 0.0439\n",
      "Epoch [10/15], Batch [50/198], Loss: 0.1468\n",
      "Epoch [10/15], Batch [60/198], Loss: 0.6538\n",
      "Epoch [10/15], Batch [70/198], Loss: 0.2693\n",
      "Epoch [10/15], Batch [80/198], Loss: 0.3087\n",
      "Epoch [10/15], Batch [90/198], Loss: 0.2091\n",
      "Epoch [10/15], Batch [100/198], Loss: 0.0479\n",
      "Epoch [10/15], Batch [110/198], Loss: 0.0964\n",
      "Epoch [10/15], Batch [120/198], Loss: 0.0794\n",
      "Epoch [10/15], Batch [130/198], Loss: 0.4813\n",
      "Epoch [10/15], Batch [140/198], Loss: 0.0941\n",
      "Epoch [10/15], Batch [150/198], Loss: 0.1181\n",
      "Epoch [10/15], Batch [160/198], Loss: 0.0899\n",
      "Epoch [10/15], Batch [170/198], Loss: 0.3500\n",
      "Epoch [10/15], Batch [180/198], Loss: 0.0439\n",
      "Epoch [10/15], Batch [190/198], Loss: 0.1943\n",
      "Epoch [10/15], Train Loss: 0.2397, Train Accuracy: 0.9322, Val Loss: 1.5818, Val Accuracy: 0.5883\n",
      "Epoch [11/15], Batch [10/198], Loss: 0.0794\n",
      "Epoch [11/15], Batch [20/198], Loss: 0.2696\n",
      "Epoch [11/15], Batch [30/198], Loss: 0.1946\n",
      "Epoch [11/15], Batch [40/198], Loss: 0.0663\n",
      "Epoch [11/15], Batch [50/198], Loss: 0.4193\n",
      "Epoch [11/15], Batch [60/198], Loss: 0.1705\n",
      "Epoch [11/15], Batch [70/198], Loss: 0.4003\n",
      "Epoch [11/15], Batch [80/198], Loss: 0.1737\n",
      "Epoch [11/15], Batch [90/198], Loss: 0.4463\n",
      "Epoch [11/15], Batch [100/198], Loss: 0.2494\n",
      "Epoch [11/15], Batch [110/198], Loss: 0.1109\n",
      "Epoch [11/15], Batch [120/198], Loss: 0.1430\n",
      "Epoch [11/15], Batch [130/198], Loss: 0.0427\n",
      "Epoch [11/15], Batch [140/198], Loss: 0.3327\n",
      "Epoch [11/15], Batch [150/198], Loss: 0.0778\n",
      "Epoch [11/15], Batch [160/198], Loss: 0.0550\n",
      "Epoch [11/15], Batch [170/198], Loss: 0.2244\n",
      "Epoch [11/15], Batch [180/198], Loss: 0.5137\n",
      "Epoch [11/15], Batch [190/198], Loss: 0.0632\n",
      "Epoch [11/15], Train Loss: 0.2115, Train Accuracy: 0.9366, Val Loss: 1.5222, Val Accuracy: 0.6045\n",
      "Early stopping triggered after 11 epochs\n",
      "Model saved to lstm_text_label_with_dropout_text_classifier.pth\n",
      "Evaluating model on test set...\n",
      "Test Loss: 1.5808, Test Accuracy: 0.6192\n",
      "Results saved to lstm_text_label_with_dropout_classification_results.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing with LSTM model...\")\n",
    "lstm_model, lstm_accuracy, lstm_loss = process_data_with_splits(model_type='lstm_text_label_with_dropout', train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca9f16b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of models:\n",
      "LTSM - Accuracy: 0.6192, Loss: 1.5808\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComparison of models:\")\n",
    "print(f\"LTSM - Accuracy: {lstm_accuracy:.4f}, Loss: {lstm_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23750444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5242-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
