{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c95eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "673876e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model\n",
    "class TextRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, num_layers=1, dropout_rate=0.8):\n",
    "        super(TextRNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(self.embedding_dim, self.hidden_size, self.num_layers, batch_first=True, dropout=self.dropout_rate)\n",
    "        \n",
    "        # Dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        # _, hidden = self.rnn(embedded, h0)\n",
    "        out, _ = self.rnn(embedded, h0)\n",
    "\n",
    "        # Apply dropout\n",
    "        # hidden = self.dropout(hidden.squeeze(0))\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        \n",
    "        # Pass the output of the last time step to the classifier\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17d66bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset for text files with labels (int[])\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_ids, labels, file_dir, tokenizer, word_to_idx, max_length=20):\n",
    "        self.file_ids = file_ids\n",
    "        self.labels = labels\n",
    "        self.file_dir = file_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_id = self.file_ids[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Read text file\n",
    "        file_path = os.path.join(self.file_dir, f\"{file_id}.txt\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            # print(\"text:\", text)\n",
    "        # Read OCR file\n",
    "        ocr_file_path = os.path.join(self.file_dir, f\"{file_id}_ocr.txt\")\n",
    "        with open(ocr_file_path, 'r', encoding='utf-8') as f:\n",
    "            ocr_text = f.readlines()\n",
    "            ocr_text = ' '.join(ocr_text)\n",
    "            # print(\"ocr_text:\", ocr_text)\n",
    "\n",
    "        # Combine text and OCR text\n",
    "        text = text + ' ' + ocr_text\n",
    "        \n",
    "        # Normalize and Tokenize\n",
    "        tokens = self.tokenizer.tokenize(text.lower())\n",
    "        \n",
    "        # Convert tokens to indices\n",
    "        indices = [self.word_to_idx.get(token, self.word_to_idx['<UNK>']) for token in tokens]\n",
    "        \n",
    "        # Truncate or pad sequence\n",
    "        if len(indices) > self.max_length:\n",
    "            indices = indices[:self.max_length]\n",
    "        else:\n",
    "            indices = indices + [self.word_to_idx['<PAD>']] * (self.max_length - len(indices))\n",
    "            \n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d52148b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from all training text files\n",
    "def build_vocabulary(file_paths, tokenizer, min_freq=2, vocab_file='vocabulary_rnn_ocr.txt'):\n",
    "    print(\"Building vocabulary...\")\n",
    "\n",
    "    # load vocabulary from file if it exists\n",
    "    if os.path.exists(vocab_file):\n",
    "        print(f\"Vocabulary file {vocab_file} already exists. Loading...\")\n",
    "        word_to_idx = {}\n",
    "        with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word, idx = line.strip().split('\\t')\n",
    "                word_to_idx[word] = int(idx)\n",
    "        return word_to_idx\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_counts = Counter()\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        tokens = tokenizer.tokenize(text.lower())\n",
    "        word_counts.update(tokens)\n",
    "    \n",
    "    # Filter words by frequency\n",
    "    words = [word for word, count in word_counts.items() if count >= min_freq]\n",
    "    \n",
    "    # Add special tokens\n",
    "    word_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for word in words:\n",
    "        word_to_idx[word] = len(word_to_idx)\n",
    "\n",
    "    # save vocabulary to file\n",
    "    with open(vocab_file, 'w', encoding='utf-8') as f:\n",
    "        for word, idx in word_to_idx.items():\n",
    "            f.write(f\"{word}\\t{idx}\\n\")\n",
    "    print(f\"Vocabulary saved to {vocab_file}\")\n",
    "    \n",
    "    return word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "436bdf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, patience=3, num_epochs=10):\n",
    "    model.to(device)\n",
    "    \n",
    "    best_val_accuracy = 0.0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_predictions = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            # print(\"training outputs:\", outputs, \"labels:\", labels)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "\n",
    "            # prevent gradient explosions in RNN\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_predictions.extend(predicted.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_predictions = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(data)\n",
    "                # print(\"outputs:\", outputs, \"labels:\", labels)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                # Get predictions\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_predictions.extend(predicted.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '\n",
    "              f'Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "                break\n",
    "    \n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, best_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ef452f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            # print(\"data:\", data, \"label:\", labels)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            print(\"outputs.data:\", outputs.data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    \n",
    "    print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "    return accuracy, avg_loss, all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ab50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_with_splits(model_type='rnn_with_dropout_with_ocr', train=True):\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'val_split_ratio': 0.15,  \n",
    "        'test_split_ratio': 0.15,\n",
    "        'seed': 42,\n",
    "        'batch_size': 16,\n",
    "        'embedding_dim': 100,\n",
    "        'hidden_size': 128,\n",
    "        'num_layers': 2,\n",
    "        'learning_rate': 0.001,\n",
    "        'num_epochs': 10,\n",
    "        'patience': 3\n",
    "    }\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize NLTK tokenizer\n",
    "    tokenizer = TweetTokenizer()\n",
    "    \n",
    "    # Load labels from CSV\n",
    "    labels_df = pd.read_csv('../../label.csv').dropna(how=\"all\")\n",
    "    print(f\"Loaded {len(labels_df)} labels from CSV\")\n",
    "\n",
    "    df = labels_df.copy().dropna(how='all')\n",
    "    df['ID'] = df['ID'].astype(int)\n",
    "    df['class'] = df['class'].astype(int)\n",
    "    \n",
    "    # # Map label text to numerical values\n",
    "    label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "    # labels_df['text_numeric_label'] = labels_df['text'].apply(lambda x: label_map.get(x.lower(), 0))\n",
    "    \n",
    "    # # Create a dataframe for splitting\n",
    "    # df = labels_df[['ID', 'text_numeric_label', 'label']].rename(columns={'text_numeric_label': 'text_label'})\n",
    "    \n",
    "    # Split the data into train, validation, and test sets\n",
    "    print(\"Splitting data...\")\n",
    "    val_test_size = config['val_split_ratio'] + config['test_split_ratio']\n",
    "    if val_test_size >= 1.0:\n",
    "        print(\"Error: Sum of validation and test split ratios must be less than 1.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Adjust test size relative to the remaining data after validation split\n",
    "    relative_test_size = config['test_split_ratio'] / (1.0 - config['val_split_ratio'])\n",
    "\n",
    "    try:\n",
    "        # Split into train and temp (val + test)\n",
    "        train_df, temp_df = train_test_split(\n",
    "            df,\n",
    "            test_size=val_test_size,\n",
    "            random_state=config['seed'],\n",
    "            stratify=df['label'] # Stratify if labels are imbalanced\n",
    "        )\n",
    "        # Split temp into val and test\n",
    "        val_df, test_df = train_test_split(\n",
    "            temp_df,\n",
    "            test_size=relative_test_size,\n",
    "            random_state=config['seed'],\n",
    "            stratify=temp_df['label'] # Stratify if labels are imbalanced\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data splitting: {e}. Check split ratios and data.\")\n",
    "        # Might happen if a label class has too few samples for stratification\n",
    "        print(\"Attempting split without stratification...\")\n",
    "        try:\n",
    "            train_df, temp_df = train_test_split(df, test_size=val_test_size, random_state=config['seed'])\n",
    "            val_df, test_df = train_test_split(temp_df, test_size=relative_test_size, random_state=config['seed'])\n",
    "        except Exception as e_nostrat:\n",
    "            print(f\"Error during non-stratified split: {e_nostrat}.\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    print(f\"Train set: {len(train_df)} samples\")\n",
    "    print(f\"Validation set: {len(val_df)} samples\")\n",
    "    print(f\"Test set: {len(test_df)} samples\")\n",
    "    \n",
    "    # Create full file paths for building vocabulary\n",
    "    train_ids = train_df['ID'].astype(int).values\n",
    "    val_ids = val_df['ID'].astype(int).values\n",
    "    test_ids = test_df['ID'].astype(int).values\n",
    "    \n",
    "    train_labels = train_df['class'].values\n",
    "    val_labels = val_df['class'].values\n",
    "    test_labels = test_df['class'].values\n",
    "    \n",
    "    # Build vocabulary from training data only to prevent data leakage\n",
    "    file_paths = [f\"../../raw_data/{id}.txt\" for id in train_ids if os.path.exists(f\"../../raw_data/{id}.txt\")]\n",
    "    file_paths.extend([f\"../../raw_data/{id}_ocr.txt\" for id in train_ids if os.path.exists(f\"../../raw_data/{id}_ocr.txt\")])\n",
    "    word_to_idx = build_vocabulary(file_paths, tokenizer)\n",
    "    vocab_size = len(word_to_idx)\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TextDataset(train_ids, train_labels, '../../raw_data', tokenizer, word_to_idx)\n",
    "    val_dataset = TextDataset(val_ids, val_labels, '../../raw_data', tokenizer, word_to_idx)\n",
    "    test_dataset = TextDataset(test_ids, test_labels, '../../raw_data', tokenizer, word_to_idx)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    \n",
    "    # Initialize the model based on type\n",
    "    num_classes = 3  # negative (0), neutral (1), positive (2)\n",
    "    # if model_type.lower() == 'lstm':\n",
    "    #     model = TextLSTM(vocab_size, config['embedding_dim'], config['hidden_size'], num_classes, config['num_layers'])\n",
    "    #     print(\"Using LSTM model\")\n",
    "    # else:\n",
    "    model = TextRNN(vocab_size, config['embedding_dim'], config['hidden_size'], num_classes, config['num_layers'])\n",
    "    print(\"Using RNN model\")\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    # Train the model if requested\n",
    "    if train:\n",
    "        print(\"Starting training...\")\n",
    "        model, best_val_acc = train_model(\n",
    "            model, \n",
    "            train_loader, \n",
    "            val_loader, \n",
    "            criterion, \n",
    "            optimizer, \n",
    "            device, \n",
    "            patience=config['patience'], \n",
    "            num_epochs=config['num_epochs']\n",
    "        )\n",
    "        \n",
    "        # Save the trained model\n",
    "        model_save_path = f\"{model_type}_text_classifier.pth\"\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'vocab': word_to_idx,\n",
    "            'config': config\n",
    "        }, model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "    else:\n",
    "        # Load pre-trained model\n",
    "        model_load_path = f\"{model_type}_text_classifier.pth\"\n",
    "        if os.path.exists(model_load_path):\n",
    "            checkpoint = torch.load(model_load_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(f\"Loaded pre-trained model from {model_load_path}\")\n",
    "        else:\n",
    "            print(f\"No pre-trained model found at {model_load_path}. Using untrained model.\")\n",
    "    \n",
    "    # Evaluate the model on test set\n",
    "    print(\"Evaluating model on test set...\")\n",
    "    test_accuracy, test_loss, test_predictions = evaluate_model(model, test_loader, criterion, device)\n",
    "    \n",
    "    # Create results dataframe for test set\n",
    "    results = pd.DataFrame({\n",
    "        'ID': test_ids,\n",
    "        'true_label': test_labels,\n",
    "        'predicted_label': test_predictions\n",
    "    })\n",
    "    \n",
    "    # Map numeric labels back to text\n",
    "    reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "    results['true_class'] = results['true_label'].map(reverse_label_map)\n",
    "    results['predicted_class'] = results['predicted_label'].map(reverse_label_map)\n",
    "    \n",
    "    # Save results\n",
    "    results.to_csv(f\"{model_type}_classification_results.csv\", index=False)\n",
    "    print(f\"Results saved to {model_type}_classification_results.csv\")\n",
    "    \n",
    "    return model, test_accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37f0f237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing with RNN model...\n",
      "Using device: cpu\n",
      "Loaded 4511 labels from CSV\n",
      "Splitting data...\n",
      "Train set: 3157 samples\n",
      "Validation set: 1115 samples\n",
      "Test set: 239 samples\n",
      "Building vocabulary...\n",
      "Vocabulary saved to vocabulary_rnn_ocr.txt\n",
      "Vocabulary size: 3994\n",
      "Using RNN model\n",
      "Starting training...\n",
      "Epoch [1/10], Batch [10/198], Loss: 1.2722\n",
      "Epoch [1/10], Batch [20/198], Loss: 1.1903\n",
      "Epoch [1/10], Batch [30/198], Loss: 1.4856\n",
      "Epoch [1/10], Batch [40/198], Loss: 1.2145\n",
      "Epoch [1/10], Batch [50/198], Loss: 0.8792\n",
      "Epoch [1/10], Batch [60/198], Loss: 1.1263\n",
      "Epoch [1/10], Batch [70/198], Loss: 1.3285\n",
      "Epoch [1/10], Batch [80/198], Loss: 1.0984\n",
      "Epoch [1/10], Batch [90/198], Loss: 1.0297\n",
      "Epoch [1/10], Batch [100/198], Loss: 0.7159\n",
      "Epoch [1/10], Batch [110/198], Loss: 1.0088\n",
      "Epoch [1/10], Batch [120/198], Loss: 0.8865\n",
      "Epoch [1/10], Batch [130/198], Loss: 1.1456\n",
      "Epoch [1/10], Batch [140/198], Loss: 1.1636\n",
      "Epoch [1/10], Batch [150/198], Loss: 1.2326\n",
      "Epoch [1/10], Batch [160/198], Loss: 0.8508\n",
      "Epoch [1/10], Batch [170/198], Loss: 0.8546\n",
      "Epoch [1/10], Batch [180/198], Loss: 0.7715\n",
      "Epoch [1/10], Batch [190/198], Loss: 0.6688\n",
      "Epoch [1/10], Train Loss: 1.0286, Train Accuracy: 0.5059, Val Loss: 0.9262, Val Accuracy: 0.5946\n",
      "Epoch [2/10], Batch [10/198], Loss: 0.9239\n",
      "Epoch [2/10], Batch [20/198], Loss: 0.9880\n",
      "Epoch [2/10], Batch [30/198], Loss: 0.8442\n",
      "Epoch [2/10], Batch [40/198], Loss: 1.3207\n",
      "Epoch [2/10], Batch [50/198], Loss: 0.7675\n",
      "Epoch [2/10], Batch [60/198], Loss: 1.0590\n",
      "Epoch [2/10], Batch [70/198], Loss: 1.0854\n",
      "Epoch [2/10], Batch [80/198], Loss: 0.9234\n",
      "Epoch [2/10], Batch [90/198], Loss: 0.7235\n",
      "Epoch [2/10], Batch [100/198], Loss: 0.9254\n",
      "Epoch [2/10], Batch [110/198], Loss: 0.9557\n",
      "Epoch [2/10], Batch [120/198], Loss: 1.2556\n",
      "Epoch [2/10], Batch [130/198], Loss: 1.0505\n",
      "Epoch [2/10], Batch [140/198], Loss: 0.9040\n",
      "Epoch [2/10], Batch [150/198], Loss: 0.8763\n",
      "Epoch [2/10], Batch [160/198], Loss: 1.1588\n",
      "Epoch [2/10], Batch [170/198], Loss: 1.1119\n",
      "Epoch [2/10], Batch [180/198], Loss: 0.7465\n",
      "Epoch [2/10], Batch [190/198], Loss: 1.3335\n",
      "Epoch [2/10], Train Loss: 0.9615, Train Accuracy: 0.5613, Val Loss: 0.9324, Val Accuracy: 0.5946\n",
      "Epoch [3/10], Batch [10/198], Loss: 1.0026\n",
      "Epoch [3/10], Batch [20/198], Loss: 0.9179\n",
      "Epoch [3/10], Batch [30/198], Loss: 1.0688\n",
      "Epoch [3/10], Batch [40/198], Loss: 0.8883\n",
      "Epoch [3/10], Batch [50/198], Loss: 0.9144\n",
      "Epoch [3/10], Batch [60/198], Loss: 0.7509\n",
      "Epoch [3/10], Batch [70/198], Loss: 1.1059\n",
      "Epoch [3/10], Batch [80/198], Loss: 1.1424\n",
      "Epoch [3/10], Batch [90/198], Loss: 0.7366\n",
      "Epoch [3/10], Batch [100/198], Loss: 1.0810\n",
      "Epoch [3/10], Batch [110/198], Loss: 1.0947\n",
      "Epoch [3/10], Batch [120/198], Loss: 0.7721\n",
      "Epoch [3/10], Batch [130/198], Loss: 0.8471\n",
      "Epoch [3/10], Batch [140/198], Loss: 1.1133\n",
      "Epoch [3/10], Batch [150/198], Loss: 1.1832\n",
      "Epoch [3/10], Batch [160/198], Loss: 1.0653\n",
      "Epoch [3/10], Batch [170/198], Loss: 0.9439\n",
      "Epoch [3/10], Batch [180/198], Loss: 1.0697\n",
      "Epoch [3/10], Batch [190/198], Loss: 0.9538\n",
      "Epoch [3/10], Train Loss: 0.9402, Train Accuracy: 0.5784, Val Loss: 0.9188, Val Accuracy: 0.5946\n",
      "Epoch [4/10], Batch [10/198], Loss: 1.0253\n",
      "Epoch [4/10], Batch [20/198], Loss: 0.7183\n",
      "Epoch [4/10], Batch [30/198], Loss: 0.9154\n",
      "Epoch [4/10], Batch [40/198], Loss: 0.9164\n",
      "Epoch [4/10], Batch [50/198], Loss: 1.0191\n",
      "Epoch [4/10], Batch [60/198], Loss: 0.8905\n",
      "Epoch [4/10], Batch [70/198], Loss: 0.8065\n",
      "Epoch [4/10], Batch [80/198], Loss: 0.9457\n",
      "Epoch [4/10], Batch [90/198], Loss: 0.8725\n",
      "Epoch [4/10], Batch [100/198], Loss: 0.6903\n",
      "Epoch [4/10], Batch [110/198], Loss: 0.8370\n",
      "Epoch [4/10], Batch [120/198], Loss: 1.1684\n",
      "Epoch [4/10], Batch [130/198], Loss: 0.8039\n",
      "Epoch [4/10], Batch [140/198], Loss: 0.9876\n",
      "Epoch [4/10], Batch [150/198], Loss: 0.8455\n",
      "Epoch [4/10], Batch [160/198], Loss: 0.9343\n",
      "Epoch [4/10], Batch [170/198], Loss: 0.7214\n",
      "Epoch [4/10], Batch [180/198], Loss: 0.7164\n",
      "Epoch [4/10], Batch [190/198], Loss: 0.9438\n",
      "Epoch [4/10], Train Loss: 0.9380, Train Accuracy: 0.5889, Val Loss: 0.9156, Val Accuracy: 0.5946\n",
      "Early stopping triggered after 4 epochs\n",
      "Model saved to rnn_with_dropout_with_ocr_text_classifier.pth\n",
      "Evaluating model on test set...\n",
      "outputs.data: tensor([[ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.3112, -1.3572,  0.8406],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.2058, -1.5112,  0.9475],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.3430, -1.4175,  0.8192],\n",
      "        [ 0.2764, -1.5438,  0.8932],\n",
      "        [ 0.2195, -1.4765,  0.9232],\n",
      "        [ 0.3363, -1.5489,  0.9253],\n",
      "        [ 0.2056, -1.4810,  0.9135],\n",
      "        [ 0.2272, -1.3772,  0.7623],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.3290, -1.4298,  0.8453],\n",
      "        [ 0.2852, -1.5592,  0.9477],\n",
      "        [ 0.2191, -1.4772,  0.9231],\n",
      "        [ 0.2192, -1.4772,  0.9231]])\n",
      "outputs.data: tensor([[ 0.2013, -1.4795,  0.9102],\n",
      "        [ 0.2761, -1.4234,  1.0014],\n",
      "        [ 0.2203, -1.4752,  0.9232],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.2191, -1.4772,  0.9231],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.1389, -1.4716,  0.7820],\n",
      "        [ 0.3316, -1.5123,  1.0777],\n",
      "        [ 0.1878, -1.4852,  0.8930],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.2894, -1.4119,  0.7182],\n",
      "        [ 0.3239, -1.4333,  1.0093],\n",
      "        [ 0.2191, -1.4769,  0.9230],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.2588, -1.3499,  0.8554]])\n",
      "outputs.data: tensor([[ 0.2165, -1.4966,  0.9117],\n",
      "        [ 0.4189, -1.3808,  1.0004],\n",
      "        [ 0.2191, -1.4741,  0.9217],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.2191, -1.4772,  0.9230],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.2074, -1.4713,  0.9121],\n",
      "        [ 0.4762, -1.3502,  0.8523],\n",
      "        [ 0.2594, -1.5817,  0.8803],\n",
      "        [ 0.2041, -1.4845,  0.9144],\n",
      "        [ 0.3842, -1.3471,  0.8885],\n",
      "        [ 0.2191, -1.4772,  0.9231],\n",
      "        [ 0.2140, -1.1460,  0.5034],\n",
      "        [ 0.2067, -1.4790,  0.9295],\n",
      "        [ 0.3551, -1.5460,  0.9292]])\n",
      "outputs.data: tensor([[ 0.2021, -1.4749,  0.9081],\n",
      "        [ 0.2387, -1.4527,  0.9395],\n",
      "        [ 0.2138, -1.5170,  0.9375],\n",
      "        [ 0.2154, -1.4734,  0.9185],\n",
      "        [ 0.1851, -1.4656,  0.8783],\n",
      "        [ 0.2469, -1.6931,  0.8591],\n",
      "        [ 0.2506, -1.2873,  0.7089],\n",
      "        [ 0.2178, -1.4745,  0.9210],\n",
      "        [ 0.2249, -1.4883,  0.9161],\n",
      "        [ 0.2167, -1.6197,  0.7279],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.3434, -1.3223,  0.7724],\n",
      "        [ 0.2730, -1.4077,  0.8341],\n",
      "        [ 0.2777, -1.3798,  1.0325],\n",
      "        [ 0.2896, -1.6049,  0.9711],\n",
      "        [ 0.2960, -1.5662,  0.9853]])\n",
      "outputs.data: tensor([[ 0.1573, -1.4709,  0.7881],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.1995, -1.4689,  0.9047],\n",
      "        [ 0.2196, -1.4768,  0.9233],\n",
      "        [ 0.2651, -1.4139,  0.9055],\n",
      "        [ 0.2506, -1.4640,  0.9134],\n",
      "        [ 0.3614, -1.5613,  1.0487],\n",
      "        [ 0.1044, -1.4512,  0.8732],\n",
      "        [ 0.3437, -1.4279,  0.9473],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.2016, -1.4771,  0.9141],\n",
      "        [ 0.1458, -1.6790,  0.7367],\n",
      "        [ 0.2462, -1.6909,  0.7422],\n",
      "        [ 0.2228, -1.5237,  0.9314],\n",
      "        [ 0.2145, -1.4732,  0.9182],\n",
      "        [ 0.3572, -1.3966,  0.8237]])\n",
      "outputs.data: tensor([[ 0.2182, -1.4761,  0.9220],\n",
      "        [ 0.2193, -1.4771,  0.9232],\n",
      "        [ 0.1704, -1.4951,  0.9135],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.2191, -1.4772,  0.9231],\n",
      "        [ 0.1501, -1.5339,  0.8701],\n",
      "        [ 0.2186, -1.4758,  0.9218],\n",
      "        [ 0.2372, -1.3581,  0.8423],\n",
      "        [ 0.3353, -1.4459,  0.8918],\n",
      "        [ 0.1389, -1.5296,  0.7784],\n",
      "        [ 0.2192, -1.4766,  0.9229],\n",
      "        [ 0.2724, -1.4298,  0.9137],\n",
      "        [ 0.1752, -1.5398,  0.9758],\n",
      "        [ 0.2143, -1.4713,  0.9175],\n",
      "        [ 0.2489, -1.6198,  1.0342],\n",
      "        [ 0.2191, -1.4765,  0.9228]])\n",
      "outputs.data: tensor([[ 0.2616, -1.5624,  0.9387],\n",
      "        [ 0.2024, -1.4734,  0.8957],\n",
      "        [ 0.2969, -1.4458,  0.8118],\n",
      "        [ 0.2264, -1.6195,  0.8940],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.3737, -1.2923,  1.0139],\n",
      "        [ 0.2189, -1.4746,  0.9216],\n",
      "        [ 0.2194, -1.4764,  0.9229],\n",
      "        [ 0.1856, -1.4744,  0.8826],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.2761, -1.4926,  0.8858],\n",
      "        [ 0.2062, -1.4681,  0.8920],\n",
      "        [ 0.2191, -1.4772,  0.9231],\n",
      "        [ 0.2405, -1.5544,  0.7425],\n",
      "        [ 0.2879, -1.4156,  0.8955],\n",
      "        [ 0.2191, -1.4772,  0.9231]])\n",
      "outputs.data: tensor([[ 0.2884, -1.4873,  0.9144],\n",
      "        [ 0.1760, -1.4412,  0.8839],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.2189, -1.4764,  0.9226],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.2043, -1.5547,  1.0006],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.3500, -1.5347,  0.9367],\n",
      "        [ 0.2933, -1.6643,  0.9254],\n",
      "        [ 0.1636, -1.3623,  0.8902],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.2109, -1.4771,  0.9169],\n",
      "        [ 0.3502, -1.3531,  0.7574],\n",
      "        [ 0.2137, -1.4775,  0.9188],\n",
      "        [ 0.2264, -1.6165,  0.7917],\n",
      "        [ 0.2046, -1.4793,  0.9089]])\n",
      "outputs.data: tensor([[ 0.3484, -1.5387,  0.9978],\n",
      "        [ 0.2194, -1.4770,  0.9232],\n",
      "        [ 0.2191, -1.4772,  0.9231],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.2188, -1.4752,  0.9222],\n",
      "        [ 0.1701, -1.4991,  0.8090],\n",
      "        [ 0.3088, -1.5868,  0.9407],\n",
      "        [ 0.2834, -1.3951,  1.0154],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.2191, -1.4772,  0.9231],\n",
      "        [ 0.2197, -1.4749,  0.9226],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.2192, -1.4766,  0.9230],\n",
      "        [ 0.2185, -1.4757,  0.9217],\n",
      "        [ 0.1736, -1.3979,  0.8759],\n",
      "        [ 0.1979, -1.7489,  0.8322]])\n",
      "outputs.data: tensor([[ 0.1605, -1.5954,  0.9707],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.3194, -1.4365,  1.0397],\n",
      "        [ 0.4237, -1.4903,  1.0126],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.2191, -1.4772,  0.9231],\n",
      "        [ 0.4306, -1.4348,  0.8989],\n",
      "        [ 0.2580, -1.5021,  0.9214],\n",
      "        [ 0.3473, -1.4638,  0.9041],\n",
      "        [ 0.3362, -1.5818,  0.8867],\n",
      "        [ 0.1475, -1.6048,  0.7013],\n",
      "        [ 0.1980, -1.4882,  0.8954],\n",
      "        [ 0.2170, -1.4844,  0.9544],\n",
      "        [ 0.2191, -1.4761,  0.9228],\n",
      "        [ 0.2191, -1.4773,  0.9231]])\n",
      "outputs.data: tensor([[ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.2192, -1.4767,  0.9229],\n",
      "        [ 0.2627, -1.4957,  0.9076],\n",
      "        [ 0.4608, -1.0566,  0.9562],\n",
      "        [ 0.2064, -1.4735,  0.9126],\n",
      "        [ 0.4610, -1.5078,  1.1063],\n",
      "        [ 0.2174, -1.4764,  0.9211],\n",
      "        [ 0.2191, -1.4772,  0.9231],\n",
      "        [ 0.2098, -1.4974,  0.9187],\n",
      "        [ 0.2191, -1.4769,  0.9229],\n",
      "        [ 0.2172, -1.4755,  0.9208],\n",
      "        [ 0.4871, -1.4718,  1.0685],\n",
      "        [ 0.2029, -1.4943,  0.9006],\n",
      "        [ 0.1988, -1.4804,  0.9073],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.2041, -1.4845,  0.9144]])\n",
      "outputs.data: tensor([[ 0.2698, -1.4195,  0.7892],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.2191, -1.4772,  0.9230],\n",
      "        [ 0.1940, -1.4579,  0.8977],\n",
      "        [ 0.2193, -1.4770,  0.9232],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.2200, -1.2615,  0.7079],\n",
      "        [ 0.1368, -1.7060,  0.7537],\n",
      "        [ 0.2191, -1.4772,  0.9231],\n",
      "        [ 0.2006, -1.4746,  0.9075],\n",
      "        [ 0.1905, -1.6038,  0.7585],\n",
      "        [ 0.2927, -1.5601,  0.8184],\n",
      "        [ 0.2154, -1.4766,  0.9202],\n",
      "        [ 0.2475, -1.5681,  1.0314],\n",
      "        [ 0.2181, -1.2549,  0.7948],\n",
      "        [ 0.2099, -1.4661,  0.9347]])\n",
      "outputs.data: tensor([[ 0.2973, -1.6599,  0.9099],\n",
      "        [ 0.2467, -1.6104,  1.0305],\n",
      "        [ 0.2034, -1.4721,  0.9084],\n",
      "        [ 0.3091, -1.4856,  0.8840],\n",
      "        [ 0.2192, -1.4772,  0.9231],\n",
      "        [ 0.3214, -1.4591,  0.9850],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.3385, -1.5050,  0.9601],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.2441, -1.4620,  1.0188],\n",
      "        [ 0.2193, -1.4769,  0.9231],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.3184, -1.3806,  0.9637],\n",
      "        [ 0.2105, -1.4758,  0.9168],\n",
      "        [ 0.2155, -1.4749,  0.9195],\n",
      "        [ 0.1418, -1.4944,  0.9111]])\n",
      "outputs.data: tensor([[ 0.2184, -1.4755,  0.9219],\n",
      "        [ 0.2846, -1.5581,  0.9930],\n",
      "        [ 0.2189, -1.4748,  0.9219],\n",
      "        [ 0.2458, -1.4823,  1.0225],\n",
      "        [ 0.2537, -1.5017,  0.9634],\n",
      "        [ 0.2134, -1.4732,  0.9170],\n",
      "        [ 0.2182, -1.4762,  0.9220],\n",
      "        [ 0.2058, -1.5112,  0.9475],\n",
      "        [ 0.1370, -1.4940,  0.7525],\n",
      "        [ 0.3353, -1.6495,  0.9650],\n",
      "        [ 0.2111, -1.3486,  0.8273],\n",
      "        [ 0.2088, -1.4812,  0.9205],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.2184, -1.4749,  0.9218],\n",
      "        [ 0.1959, -1.4659,  0.9018],\n",
      "        [ 0.1331, -1.5240,  0.9270]])\n",
      "outputs.data: tensor([[ 0.2171, -1.4945,  0.9289],\n",
      "        [ 0.4493, -1.5465,  0.9212],\n",
      "        [ 0.2191, -1.4772,  0.9230],\n",
      "        [ 0.2637, -1.5058,  0.9462],\n",
      "        [ 0.2194, -1.4748,  0.9223],\n",
      "        [ 0.2016, -1.4756,  0.9103],\n",
      "        [ 0.3184, -1.2479,  0.7827],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.3390, -1.4538,  1.0814],\n",
      "        [ 0.3371, -1.3973,  0.8121],\n",
      "        [ 0.2053, -1.4908,  0.9029],\n",
      "        [ 0.2098, -1.4793,  0.9134],\n",
      "        [ 0.2191, -1.4773,  0.9231],\n",
      "        [ 0.1971, -1.4864,  0.9117],\n",
      "        [ 0.4113, -1.3662,  0.9334]])\n",
      "Test Loss: 0.9153, Test Accuracy: 0.5941\n",
      "Results saved to rnn_with_dropout_with_ocr_classification_results.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing with RNN model...\")\n",
    "rnn_model, rnn_accuracy, rnn_loss = process_data_with_splits(model_type='rnn_with_dropout_with_ocr', train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca9f16b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of models:\n",
      "RNN - Accuracy: 0.5941, Loss: 0.9153\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComparison of models:\")\n",
    "print(f\"RNN - Accuracy: {rnn_accuracy:.4f}, Loss: {rnn_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23750444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5242-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
