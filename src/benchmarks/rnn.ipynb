{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c95eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "673876e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model\n",
    "class TextRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, num_layers=1):\n",
    "        super(TextRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        out, hidden = self.rnn(embedded, h0)\n",
    "        # out, hidden = self.rnn(embedded)\n",
    "        # print(\"out.shape:\", out.shape)\n",
    "        # print(\"hidden.shape:\", hidden.shape)\n",
    "        \n",
    "        # Pass the output of the last time step to the classifier\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        # out = self.fc(out)\n",
    "        # out = self.fc(hidden.squeeze(0))\n",
    "        # print(\"out.shape:\", out.shape)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17d66bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset for text files with labels\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_ids, labels, file_dir, tokenizer, word_to_idx, max_length=20):\n",
    "        self.file_ids = file_ids\n",
    "        self.labels = labels\n",
    "        self.file_dir = file_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_id = self.file_ids[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Read text file\n",
    "        file_path = os.path.join(self.file_dir, f\"{file_id}.txt\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        # Normalize and Tokenize\n",
    "        tokens = self.tokenizer.tokenize(text.lower())\n",
    "        \n",
    "        # Convert tokens to indices\n",
    "        indices = [self.word_to_idx.get(token, self.word_to_idx['<UNK>']) for token in tokens]\n",
    "        \n",
    "        # Truncate or pad sequence\n",
    "        if len(indices) > self.max_length:\n",
    "            indices = indices[:self.max_length]\n",
    "        else:\n",
    "            indices = indices + [self.word_to_idx['<PAD>']] * (self.max_length - len(indices))\n",
    "            \n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d52148b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from all text files\n",
    "def build_vocabulary(file_paths, tokenizer, min_freq=2, vocab_file='vocabulary_rnn.txt'):\n",
    "    print(\"Building vocabulary...\")\n",
    "\n",
    "    # load vocabulary from file if it exists\n",
    "    if os.path.exists(vocab_file):\n",
    "        print(f\"Vocabulary file {vocab_file} already exists. Loading...\")\n",
    "        word_to_idx = {}\n",
    "        with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word, idx = line.strip().split('\\t')\n",
    "                word_to_idx[word] = int(idx)\n",
    "        return word_to_idx\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_counts = Counter()\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        tokens = tokenizer.tokenize(text.lower())\n",
    "        word_counts.update(tokens)\n",
    "    \n",
    "    # Filter words by frequency\n",
    "    words = [word for word, count in word_counts.items() if count >= min_freq]\n",
    "    \n",
    "    # Add special tokens\n",
    "    word_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for word in words:\n",
    "        word_to_idx[word] = len(word_to_idx)\n",
    "\n",
    "    # save vocabulary to file\n",
    "    with open(vocab_file, 'w', encoding='utf-8') as f:\n",
    "        for word, idx in word_to_idx.items():\n",
    "            f.write(f\"{word}\\t{idx}\\n\")\n",
    "    print(f\"Vocabulary saved to {vocab_file}\")\n",
    "    \n",
    "    return word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "436bdf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, patience=3, num_epochs=10):\n",
    "    model.to(device)\n",
    "    \n",
    "    best_val_accuracy = 0.0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_predictions = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            # print(\"data:\", data)\n",
    "            # print(\"training outputs:\", outputs)\n",
    "            # print all parameters in the model\n",
    "            # for name, param in model.named_parameters():\n",
    "            #     print(f\"{name}: {param.data}, {param.requires_grad}\")\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            \n",
    "            # prevent gradient explosions in RNN\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_predictions.extend(predicted.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_predictions = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(data)\n",
    "                # print(\"outputs:\", outputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                # Get predictions\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_predictions.extend(predicted.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '\n",
    "              f'Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "                break\n",
    "    \n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, best_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ef452f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            print(\"outputs.data\", outputs.data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    \n",
    "    print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "    return accuracy, avg_loss, all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "475ab50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_with_splits(model_type='rnn', train=True):\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'val_split_ratio': 0.15,  \n",
    "        'test_split_ratio': 0.15,\n",
    "        'seed': 42,\n",
    "        'batch_size': 16,\n",
    "        'embedding_dim': 100,\n",
    "        'hidden_size': 128,\n",
    "        'num_layers': 2,\n",
    "        'learning_rate': 0.001,\n",
    "        'num_epochs': 10,\n",
    "        'patience': 3\n",
    "    }\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize NLTK tokenizer\n",
    "    tokenizer = TweetTokenizer()\n",
    "    \n",
    "    # Load labels from CSV\n",
    "    labels_df = pd.read_csv('../../label.csv')\n",
    "    print(f\"Loaded {len(labels_df)} labels from CSV\")\n",
    "\n",
    "    df = labels_df.copy().dropna(how='all')\n",
    "    df['ID'] = df['ID'].astype(int)\n",
    "    df['class'] = df['class'].astype(int)\n",
    "    \n",
    "    # # Map label text to numerical values\n",
    "    label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "    # labels_df['text_numeric_label'] = labels_df['class'].apply(lambda x: label_map.get(x.lower(), 0))\n",
    "    \n",
    "    # # Create a dataframe for splitting\n",
    "    # df = labels_df[['ID', 'text_numeric_label']].rename(columns={'numeric_label': 'text_label'})\n",
    "    \n",
    "    # Split the data into train, validation, and test sets\n",
    "    print(\"Splitting data...\")\n",
    "    val_test_size = config['val_split_ratio'] + config['test_split_ratio']\n",
    "    if val_test_size >= 1.0:\n",
    "        print(\"Error: Sum of validation and test split ratios must be less than 1.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Adjust test size relative to the remaining data after validation split\n",
    "    relative_test_size = config['test_split_ratio'] / (1.0 - config['val_split_ratio'])\n",
    "\n",
    "    try:\n",
    "        # Split into train and temp (val + test)\n",
    "        train_df, temp_df = train_test_split(\n",
    "            df,\n",
    "            test_size=val_test_size,\n",
    "            random_state=config['seed'],\n",
    "            stratify=df['label'] # Stratify if labels are imbalanced\n",
    "        )\n",
    "        # Split temp into val and test\n",
    "        val_df, test_df = train_test_split(\n",
    "            temp_df,\n",
    "            test_size=relative_test_size,\n",
    "            random_state=config['seed'],\n",
    "            stratify=temp_df['label'] # Stratify if labels are imbalanced\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data splitting: {e}. Check split ratios and data.\")\n",
    "        # Might happen if a label class has too few samples for stratification\n",
    "        print(\"Attempting split without stratification...\")\n",
    "        try:\n",
    "            train_df, temp_df = train_test_split(df, test_size=val_test_size, random_state=config['seed'])\n",
    "            val_df, test_df = train_test_split(temp_df, test_size=relative_test_size, random_state=config['seed'])\n",
    "        except Exception as e_nostrat:\n",
    "            print(f\"Error during non-stratified split: {e_nostrat}.\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    print(f\"Train set: {len(train_df)} samples\")\n",
    "    print(f\"Validation set: {len(val_df)} samples\")\n",
    "    print(f\"Test set: {len(test_df)} samples\")\n",
    "    \n",
    "    # Create full file paths for building vocabulary\n",
    "    train_ids = train_df['ID'].astype(int).values\n",
    "    val_ids = val_df['ID'].astype(int).values\n",
    "    test_ids = test_df['ID'].astype(int).values\n",
    "    \n",
    "    train_labels = train_df['class'].values\n",
    "    val_labels = val_df['class'].values\n",
    "    test_labels = test_df['class'].values\n",
    "    \n",
    "    # Build vocabulary from training data only to prevent data leakage\n",
    "    file_paths = [f\"../../raw_data/{id}.txt\" for id in train_ids if os.path.exists(f\"../../raw_data/{id}.txt\")]\n",
    "    word_to_idx = build_vocabulary(file_paths, tokenizer)\n",
    "    vocab_size = len(word_to_idx)\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TextDataset(train_ids, train_labels, '../../raw_data', tokenizer, word_to_idx)\n",
    "    val_dataset = TextDataset(val_ids, val_labels, '../../raw_data', tokenizer, word_to_idx)\n",
    "    test_dataset = TextDataset(test_ids, test_labels, '../../raw_data', tokenizer, word_to_idx)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    \n",
    "    # Initialize the model based on type\n",
    "    num_classes = 3  # negative (0), neutral (1), positive (2)\n",
    "    # if model_type.lower() == 'lstm':\n",
    "    #     model = TextLSTM(vocab_size, config['embedding_dim'], config['hidden_size'], num_classes, config['num_layers'])\n",
    "    #     print(\"Using LSTM model\")\n",
    "    # else:\n",
    "    model = TextRNN(vocab_size, config['embedding_dim'], config['hidden_size'], num_classes, config['num_layers'])\n",
    "    print(\"Using RNN model\")\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    # Train the model if requested\n",
    "    if train:\n",
    "        print(\"Starting training...\")\n",
    "        model, best_val_acc = train_model(\n",
    "            model, \n",
    "            train_loader, \n",
    "            val_loader, \n",
    "            criterion, \n",
    "            optimizer, \n",
    "            device, \n",
    "            patience=config['patience'], \n",
    "            num_epochs=config['num_epochs']\n",
    "        )\n",
    "        \n",
    "        # Save the trained model\n",
    "        model_save_path = f\"{model_type}_text_classifier.pth\"\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'vocab': word_to_idx,\n",
    "            'config': config\n",
    "        }, model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "    else:\n",
    "        # Load pre-trained model\n",
    "        model_load_path = f\"{model_type}_text_classifier.pth\"\n",
    "        if os.path.exists(model_load_path):\n",
    "            checkpoint = torch.load(model_load_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(f\"Loaded pre-trained model from {model_load_path}\")\n",
    "        else:\n",
    "            print(f\"No pre-trained model found at {model_load_path}. Using untrained model.\")\n",
    "    \n",
    "    # Evaluate the model on test set\n",
    "    print(\"Evaluating model on test set...\")\n",
    "    test_accuracy, test_loss, test_predictions = evaluate_model(model, test_loader, criterion, device)\n",
    "    \n",
    "    # Create results dataframe for test set\n",
    "    results = pd.DataFrame({\n",
    "        'ID': test_ids,\n",
    "        'true_label': test_labels,\n",
    "        'predicted_label': test_predictions\n",
    "    })\n",
    "    \n",
    "    # Map numeric labels back to text\n",
    "    reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "    results['true_class'] = results['true_label'].map(reverse_label_map)\n",
    "    results['predicted_class'] = results['predicted_label'].map(reverse_label_map)\n",
    "    \n",
    "    # Save results\n",
    "    results.to_csv(f\"{model_type}_classification_results.csv\", index=False)\n",
    "    print(f\"Results saved to {model_type}_classification_results.csv\")\n",
    "    \n",
    "    return model, test_accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37f0f237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing with RNN model...\n",
      "Using device: cpu\n",
      "Loaded 4869 labels from CSV\n",
      "Splitting data...\n",
      "Train set: 3157 samples\n",
      "Validation set: 1115 samples\n",
      "Test set: 239 samples\n",
      "Building vocabulary...\n",
      "Vocabulary saved to vocabulary_rnn.txt\n",
      "Vocabulary size: 3703\n",
      "Using RNN model\n",
      "Starting training...\n",
      "Epoch [1/10], Batch [10/198], Loss: 0.9150\n",
      "Epoch [1/10], Batch [20/198], Loss: 0.9511\n",
      "Epoch [1/10], Batch [30/198], Loss: 0.9144\n",
      "Epoch [1/10], Batch [40/198], Loss: 0.7571\n",
      "Epoch [1/10], Batch [50/198], Loss: 0.5812\n",
      "Epoch [1/10], Batch [60/198], Loss: 0.9517\n",
      "Epoch [1/10], Batch [70/198], Loss: 0.7645\n",
      "Epoch [1/10], Batch [80/198], Loss: 1.0387\n",
      "Epoch [1/10], Batch [90/198], Loss: 1.1951\n",
      "Epoch [1/10], Batch [100/198], Loss: 0.8594\n",
      "Epoch [1/10], Batch [110/198], Loss: 0.9827\n",
      "Epoch [1/10], Batch [120/198], Loss: 0.7845\n",
      "Epoch [1/10], Batch [130/198], Loss: 0.9248\n",
      "Epoch [1/10], Batch [140/198], Loss: 1.1012\n",
      "Epoch [1/10], Batch [150/198], Loss: 0.6697\n",
      "Epoch [1/10], Batch [160/198], Loss: 0.8987\n",
      "Epoch [1/10], Batch [170/198], Loss: 0.8305\n",
      "Epoch [1/10], Batch [180/198], Loss: 0.6726\n",
      "Epoch [1/10], Batch [190/198], Loss: 1.1369\n",
      "Epoch [1/10], Train Loss: 0.9403, Train Accuracy: 0.5854, Val Loss: 0.9319, Val Accuracy: 0.5928\n",
      "Epoch [2/10], Batch [10/198], Loss: 0.9029\n",
      "Epoch [2/10], Batch [20/198], Loss: 0.8128\n",
      "Epoch [2/10], Batch [30/198], Loss: 1.0109\n",
      "Epoch [2/10], Batch [40/198], Loss: 1.0527\n",
      "Epoch [2/10], Batch [50/198], Loss: 0.8133\n",
      "Epoch [2/10], Batch [60/198], Loss: 0.8810\n",
      "Epoch [2/10], Batch [70/198], Loss: 0.6999\n",
      "Epoch [2/10], Batch [80/198], Loss: 0.7938\n",
      "Epoch [2/10], Batch [90/198], Loss: 0.9769\n",
      "Epoch [2/10], Batch [100/198], Loss: 0.7414\n",
      "Epoch [2/10], Batch [110/198], Loss: 0.8369\n",
      "Epoch [2/10], Batch [120/198], Loss: 0.7523\n",
      "Epoch [2/10], Batch [130/198], Loss: 0.5812\n",
      "Epoch [2/10], Batch [140/198], Loss: 1.3121\n",
      "Epoch [2/10], Batch [150/198], Loss: 0.9074\n",
      "Epoch [2/10], Batch [160/198], Loss: 0.8759\n",
      "Epoch [2/10], Batch [170/198], Loss: 0.6782\n",
      "Epoch [2/10], Batch [180/198], Loss: 0.7878\n",
      "Epoch [2/10], Batch [190/198], Loss: 0.8206\n",
      "Epoch [2/10], Train Loss: 0.9014, Train Accuracy: 0.5942, Val Loss: 0.9182, Val Accuracy: 0.5830\n",
      "Epoch [3/10], Batch [10/198], Loss: 0.7892\n",
      "Epoch [3/10], Batch [20/198], Loss: 1.0202\n",
      "Epoch [3/10], Batch [30/198], Loss: 0.7965\n",
      "Epoch [3/10], Batch [40/198], Loss: 0.7346\n",
      "Epoch [3/10], Batch [50/198], Loss: 0.5705\n",
      "Epoch [3/10], Batch [60/198], Loss: 0.7515\n",
      "Epoch [3/10], Batch [70/198], Loss: 1.2409\n",
      "Epoch [3/10], Batch [80/198], Loss: 0.8601\n",
      "Epoch [3/10], Batch [90/198], Loss: 0.9445\n",
      "Epoch [3/10], Batch [100/198], Loss: 0.8083\n",
      "Epoch [3/10], Batch [110/198], Loss: 0.8762\n",
      "Epoch [3/10], Batch [120/198], Loss: 1.0710\n",
      "Epoch [3/10], Batch [130/198], Loss: 0.7735\n",
      "Epoch [3/10], Batch [140/198], Loss: 0.8973\n",
      "Epoch [3/10], Batch [150/198], Loss: 0.7003\n",
      "Epoch [3/10], Batch [160/198], Loss: 0.8786\n",
      "Epoch [3/10], Batch [170/198], Loss: 1.0413\n",
      "Epoch [3/10], Batch [180/198], Loss: 1.0731\n",
      "Epoch [3/10], Batch [190/198], Loss: 0.6719\n",
      "Epoch [3/10], Train Loss: 0.8747, Train Accuracy: 0.6088, Val Loss: 0.9167, Val Accuracy: 0.5848\n",
      "Epoch [4/10], Batch [10/198], Loss: 0.8581\n",
      "Epoch [4/10], Batch [20/198], Loss: 0.5044\n",
      "Epoch [4/10], Batch [30/198], Loss: 0.7282\n",
      "Epoch [4/10], Batch [40/198], Loss: 0.8024\n",
      "Epoch [4/10], Batch [50/198], Loss: 0.8355\n",
      "Epoch [4/10], Batch [60/198], Loss: 0.7052\n",
      "Epoch [4/10], Batch [70/198], Loss: 0.8945\n",
      "Epoch [4/10], Batch [80/198], Loss: 0.8095\n",
      "Epoch [4/10], Batch [90/198], Loss: 0.8652\n",
      "Epoch [4/10], Batch [100/198], Loss: 0.8202\n",
      "Epoch [4/10], Batch [110/198], Loss: 1.2864\n",
      "Epoch [4/10], Batch [120/198], Loss: 1.1594\n",
      "Epoch [4/10], Batch [130/198], Loss: 0.7271\n",
      "Epoch [4/10], Batch [140/198], Loss: 0.6576\n",
      "Epoch [4/10], Batch [150/198], Loss: 0.7114\n",
      "Epoch [4/10], Batch [160/198], Loss: 1.0197\n",
      "Epoch [4/10], Batch [170/198], Loss: 0.8978\n",
      "Epoch [4/10], Batch [180/198], Loss: 0.8680\n",
      "Epoch [4/10], Batch [190/198], Loss: 0.7628\n",
      "Epoch [4/10], Train Loss: 0.8084, Train Accuracy: 0.6367, Val Loss: 0.9852, Val Accuracy: 0.5525\n",
      "Early stopping triggered after 4 epochs\n",
      "Model saved to rnn_text_classifier.pth\n",
      "Evaluating model on test set...\n",
      "outputs.data tensor([[-2.4044e-01, -1.6453e+00,  3.3239e-01],\n",
      "        [-4.7262e-01, -1.4092e+00,  4.4627e-01],\n",
      "        [-2.6147e-01, -1.6336e+00,  3.1820e-01],\n",
      "        [-4.5779e-01, -1.3822e+00,  7.2565e-01],\n",
      "        [-2.0098e-01, -1.6635e+00,  3.1271e-01],\n",
      "        [-2.6124e-04, -1.9314e+00,  1.8069e+00],\n",
      "        [-4.0017e-01, -1.6073e+00,  9.2717e-01],\n",
      "        [-4.4030e-01, -1.7659e+00,  4.8452e-01],\n",
      "        [ 2.6230e-01, -8.6901e-01, -6.7819e-02],\n",
      "        [-6.4279e-02, -1.4714e+00,  2.9208e-02],\n",
      "        [ 1.5916e+00, -1.5581e+00, -1.4174e-02],\n",
      "        [-2.2622e-01, -1.6600e+00,  3.4194e-01],\n",
      "        [ 3.6790e-01, -3.0648e+00,  1.7799e+00],\n",
      "        [-1.5275e-01, -7.0107e-01, -4.1417e-02],\n",
      "        [-2.4640e-01, -1.5995e+00,  2.5600e-01],\n",
      "        [-2.3592e-01, -1.6292e+00,  2.8959e-01]])\n",
      "outputs.data tensor([[-0.5502, -1.4048,  0.3573],\n",
      "        [-0.7493, -1.7827,  0.5471],\n",
      "        [-0.0324, -1.6431,  0.2137],\n",
      "        [-0.2095, -1.6694,  0.3380],\n",
      "        [-0.2214, -1.6645,  0.3408],\n",
      "        [-0.2164, -1.6893,  0.3949],\n",
      "        [ 0.3763, -2.0599,  0.7222],\n",
      "        [-0.8209, -2.2755,  1.5726],\n",
      "        [-0.5484, -1.7345,  0.4033],\n",
      "        [-0.2497, -1.6300,  0.3194],\n",
      "        [-0.2527, -1.6132,  0.2782],\n",
      "        [ 0.3832, -1.3091,  0.1447],\n",
      "        [ 2.4455, -1.4774, -0.2223],\n",
      "        [-0.2897, -1.7111,  0.4447],\n",
      "        [-0.2495, -1.6250,  0.3028],\n",
      "        [-0.5427, -2.7474,  2.2582]])\n",
      "outputs.data tensor([[ 0.3058, -1.6638,  0.2994],\n",
      "        [ 1.3508, -1.4720,  0.3321],\n",
      "        [ 0.2474, -1.7891,  0.3581],\n",
      "        [-0.2595, -1.6240,  0.3181],\n",
      "        [-0.2430, -1.6486,  0.3368],\n",
      "        [-0.2593, -1.6182,  0.3010],\n",
      "        [-0.2655, -1.6211,  0.3153],\n",
      "        [ 0.0414, -1.6683,  0.2530],\n",
      "        [ 2.0821, -1.2099, -0.1771],\n",
      "        [-0.3254, -2.7950,  1.6443],\n",
      "        [-0.6896, -1.3900,  0.2688],\n",
      "        [ 0.4847, -2.4558,  1.0970],\n",
      "        [-0.2418, -1.6818,  0.3819],\n",
      "        [-2.4259, -0.4318,  1.9862],\n",
      "        [-0.0930, -1.7552,  0.2197],\n",
      "        [-0.0847, -1.0518,  0.9674]])\n",
      "outputs.data tensor([[-2.6132e-01, -1.6515e+00,  3.8479e-01],\n",
      "        [ 2.6980e-01, -1.5754e+00,  1.1053e-01],\n",
      "        [-3.9651e-01, -1.5663e+00,  2.9613e-01],\n",
      "        [-3.6533e-01, -1.7263e+00,  3.4088e-01],\n",
      "        [-2.3604e-01, -2.1502e+00,  8.3476e-01],\n",
      "        [ 1.8702e+00, -2.5317e+00,  4.8478e-01],\n",
      "        [-8.1325e-01, -1.0340e+00,  1.1196e+00],\n",
      "        [-8.2932e-02, -1.6603e+00,  3.4612e-01],\n",
      "        [ 1.7524e-03, -1.8211e+00,  4.8469e-01],\n",
      "        [ 5.5566e-01, -2.2381e+00,  8.4321e-01],\n",
      "        [-2.0679e-01, -1.6784e+00,  3.4565e-01],\n",
      "        [ 2.3739e+00, -1.8935e+00,  2.3300e-01],\n",
      "        [ 2.0560e-02, -2.2014e+00,  1.0754e+00],\n",
      "        [ 6.6346e-01, -2.1788e+00,  7.6122e-01],\n",
      "        [-1.0572e+00, -7.4332e-01,  1.0657e+00],\n",
      "        [ 1.9325e+00, -2.6417e+00,  7.2432e-01]])\n",
      "outputs.data tensor([[ 1.6692, -1.5030, -0.1134],\n",
      "        [-0.1825, -1.6886,  0.3422],\n",
      "        [-0.2714, -1.4724,  0.1978],\n",
      "        [-0.1675, -1.6914,  0.3474],\n",
      "        [ 1.2780, -1.5414,  0.0395],\n",
      "        [-0.5078, -1.1269,  0.4151],\n",
      "        [-0.7558, -1.3070,  1.1699],\n",
      "        [-1.0862, -1.0319,  1.2732],\n",
      "        [-0.3760, -2.5005,  1.8817],\n",
      "        [-0.2190, -1.6715,  0.3503],\n",
      "        [-0.4949, -1.5506,  0.4298],\n",
      "        [-0.1910, -2.1428,  1.0582],\n",
      "        [-0.3937, -2.0315,  0.6266],\n",
      "        [-0.0793, -1.7458,  0.0846],\n",
      "        [-0.1127, -1.8232,  0.4075],\n",
      "        [ 0.2283, -2.4218,  1.2454]])\n",
      "outputs.data tensor([[-0.2307, -1.6746,  0.4399],\n",
      "        [-0.1652, -1.6680,  0.3861],\n",
      "        [ 0.6815, -1.8531,  0.5213],\n",
      "        [-0.2190, -1.6655,  0.3377],\n",
      "        [-0.2801, -1.5937,  0.2708],\n",
      "        [ 0.2199, -3.0402,  1.7229],\n",
      "        [ 0.2571, -1.7686,  0.2611],\n",
      "        [-0.5065, -1.8317,  0.5714],\n",
      "        [ 0.7981, -2.0776,  0.6568],\n",
      "        [-0.2304, -1.6751,  0.3716],\n",
      "        [-0.3717, -1.7507,  0.5004],\n",
      "        [-0.5829, -2.0641,  0.8348],\n",
      "        [-1.5797,  0.2291,  0.5988],\n",
      "        [-0.5408, -1.5757,  0.3837],\n",
      "        [-1.1981, -3.0073,  2.4138],\n",
      "        [-0.2710, -1.7596,  0.4393]])\n",
      "outputs.data tensor([[-2.0273, -1.7389,  2.2096],\n",
      "        [-0.0805, -1.4499,  0.0763],\n",
      "        [-0.0563, -1.1476,  0.1650],\n",
      "        [-0.5772, -1.2783,  0.7131],\n",
      "        [-0.2110, -1.6740,  0.3444],\n",
      "        [ 0.3598, -0.7393,  0.5112],\n",
      "        [-0.0066, -1.6148,  0.1566],\n",
      "        [ 0.0439, -1.7643,  0.3646],\n",
      "        [ 0.3711, -1.9287,  0.2110],\n",
      "        [-0.1768, -1.7047,  0.3568],\n",
      "        [-0.2212, -1.7973,  0.4253],\n",
      "        [ 0.1684, -1.6744,  0.1912],\n",
      "        [-0.1648, -1.6930,  0.3198],\n",
      "        [ 0.7811, -0.9052, -0.2523],\n",
      "        [ 0.8152, -1.7106,  0.6275],\n",
      "        [-0.2181, -1.6535,  0.2949]])\n",
      "outputs.data tensor([[ 0.2491, -1.7772,  0.6001],\n",
      "        [-0.6346, -1.5828,  0.6124],\n",
      "        [-0.2423, -1.6568,  0.3333],\n",
      "        [-0.3042, -1.6371,  0.3218],\n",
      "        [-0.2590, -1.6407,  0.3411],\n",
      "        [-0.1733, -1.6430,  0.5353],\n",
      "        [-0.1994, -1.6697,  0.3285],\n",
      "        [-0.2746, -1.5936,  0.2818],\n",
      "        [-0.3704, -2.4643,  1.6034],\n",
      "        [ 0.5507, -1.4763,  0.1449],\n",
      "        [-0.2208, -1.6673,  0.3416],\n",
      "        [ 0.1149, -1.8063,  0.4399],\n",
      "        [-0.2514, -1.6551,  0.3551],\n",
      "        [-0.5794, -1.5812,  0.3896],\n",
      "        [ 0.4794, -1.7566,  0.8143],\n",
      "        [-0.1501, -1.5985,  0.3698]])\n",
      "outputs.data tensor([[ 0.3279, -2.0493,  0.6542],\n",
      "        [-0.1231, -1.7182,  0.3510],\n",
      "        [-0.2524, -1.6415,  0.3457],\n",
      "        [-0.2286, -1.6658,  0.3493],\n",
      "        [ 0.0836, -1.7675,  0.4130],\n",
      "        [ 0.4586, -1.9192,  1.0149],\n",
      "        [-1.0768, -1.7009,  1.2643],\n",
      "        [ 0.1614, -2.5362,  1.2294],\n",
      "        [-0.1787, -1.7045,  0.3596],\n",
      "        [-0.2526, -1.6292,  0.3367],\n",
      "        [-0.4491, -1.5708,  0.3949],\n",
      "        [-0.1786, -1.6761,  0.3170],\n",
      "        [-0.3934, -1.5992,  0.2985],\n",
      "        [ 0.1902, -1.7333,  0.2545],\n",
      "        [-0.2910, -1.5777,  0.2721],\n",
      "        [-0.7351, -2.5177,  1.7978]])\n",
      "outputs.data tensor([[-1.6076, -2.2986,  2.1241],\n",
      "        [-0.2483, -1.6804,  0.3572],\n",
      "        [-0.2315, -1.6500,  0.3283],\n",
      "        [-0.1044, -2.9898,  1.9691],\n",
      "        [ 0.6963, -1.3837,  0.7025],\n",
      "        [-0.2131, -1.6695,  0.3423],\n",
      "        [-0.1220, -1.7374,  0.3653],\n",
      "        [ 0.5173, -1.8807,  0.6057],\n",
      "        [ 0.3501, -2.7932,  1.6464],\n",
      "        [ 0.9475, -1.4662,  0.0265],\n",
      "        [ 1.1180, -1.8449,  0.3561],\n",
      "        [-1.1815,  0.8588, -0.2006],\n",
      "        [-0.4750, -1.5719,  0.3838],\n",
      "        [-0.0302, -2.1441,  0.7615],\n",
      "        [-0.2722, -1.7433,  0.4195],\n",
      "        [-0.3324, -1.6404,  0.3547]])\n",
      "outputs.data tensor([[-0.2153, -1.6682,  0.3337],\n",
      "        [-0.3034, -1.5887,  0.3134],\n",
      "        [-1.1702, -1.6137,  1.3682],\n",
      "        [-0.7720, -2.5155,  2.2799],\n",
      "        [-0.2814, -1.5865,  0.2787],\n",
      "        [ 0.2509, -2.7504,  1.6841],\n",
      "        [-0.3030, -1.7368,  0.3676],\n",
      "        [-0.2224, -1.6485,  0.3158],\n",
      "        [-0.5051, -1.5513,  0.2793],\n",
      "        [-0.2022, -1.7495,  0.4345],\n",
      "        [-0.0638, -1.7249,  0.3894],\n",
      "        [ 0.3124, -2.6497,  1.5041],\n",
      "        [-0.2530, -1.6826,  0.3257],\n",
      "        [-0.1348, -1.6971,  0.3323],\n",
      "        [-0.1746, -1.7128,  0.3834],\n",
      "        [-0.6896, -1.3900,  0.2688]])\n",
      "outputs.data tensor([[ 0.4958, -0.9080,  0.1846],\n",
      "        [-0.2315, -1.6500,  0.3283],\n",
      "        [-0.2802, -1.6605,  0.3916],\n",
      "        [-0.2592, -2.0186,  0.7247],\n",
      "        [-0.2085, -1.6814,  0.3643],\n",
      "        [-0.1691, -1.7340,  0.4145],\n",
      "        [ 1.0687, -1.9287,  0.3731],\n",
      "        [-0.6459, -2.4671,  2.1641],\n",
      "        [-0.2156, -1.6871,  0.3759],\n",
      "        [-0.2822, -1.5559,  0.2688],\n",
      "        [-0.5533, -1.7235,  1.1211],\n",
      "        [ 1.1453, -1.5856,  0.1934],\n",
      "        [-0.1587, -1.7064,  0.3641],\n",
      "        [ 1.3140, -1.5891,  0.3920],\n",
      "        [-1.2447, -1.4997,  1.1100],\n",
      "        [ 0.8070, -1.4542, -0.0364]])\n",
      "outputs.data tensor([[ 6.4449e-01, -3.2604e+00,  1.4990e+00],\n",
      "        [ 1.2665e+00, -1.3008e+00, -4.1833e-01],\n",
      "        [ 1.6253e-01, -1.7110e+00,  2.6945e-01],\n",
      "        [-2.0987e+00, -4.0981e-01,  1.2240e+00],\n",
      "        [-2.3156e-01, -1.6244e+00,  2.7654e-01],\n",
      "        [ 3.4470e-01, -2.8086e+00,  1.2554e+00],\n",
      "        [-2.2373e-01, -1.6751e+00,  3.6403e-01],\n",
      "        [-1.3823e-01, -1.6197e+00,  2.3560e-01],\n",
      "        [-2.1672e-01, -1.6646e+00,  3.3806e-01],\n",
      "        [-3.1433e-01, -2.3201e+00,  1.3363e+00],\n",
      "        [-1.9156e-01, -1.6560e+00,  3.2960e-01],\n",
      "        [-1.8161e-01, -1.6523e+00,  2.8633e-01],\n",
      "        [ 8.0210e-01, -2.6146e+00,  1.0135e+00],\n",
      "        [-7.2772e-04, -1.6185e+00,  3.4655e-01],\n",
      "        [-2.4310e-01, -1.6593e+00,  3.7323e-01],\n",
      "        [-4.7692e-01, -2.5548e+00,  1.7641e+00]])\n",
      "outputs.data tensor([[-0.1048, -1.6258,  0.3209],\n",
      "        [ 0.0428,  0.0990,  0.1836],\n",
      "        [-0.0854, -1.6772,  0.3805],\n",
      "        [ 0.0304, -1.7512,  0.8700],\n",
      "        [ 1.3223, -1.3582, -0.1444],\n",
      "        [-0.7151, -1.5239,  0.4087],\n",
      "        [-0.0399, -1.7809,  0.4902],\n",
      "        [-0.4578, -1.3822,  0.7256],\n",
      "        [ 2.3338, -2.0062,  0.0044],\n",
      "        [-0.5427, -2.9758,  2.0603],\n",
      "        [-1.4025, -0.4377,  1.1557],\n",
      "        [-0.5331, -1.5833,  0.2607],\n",
      "        [-0.2208, -1.6673,  0.3416],\n",
      "        [-0.1004, -1.6021,  0.2688],\n",
      "        [-0.2085, -1.8790,  0.6919],\n",
      "        [-0.0674, -1.6458,  0.9276]])\n",
      "outputs.data tensor([[-0.4760, -1.2420,  0.1024],\n",
      "        [ 1.5503, -1.3870,  0.0801],\n",
      "        [-0.2159, -1.6911,  0.3797],\n",
      "        [-0.2606, -1.3004,  0.5447],\n",
      "        [ 0.2184, -1.6172,  0.1389],\n",
      "        [-0.1644, -2.0256,  0.5516],\n",
      "        [-0.8482, -0.0513,  0.0958],\n",
      "        [-0.1651, -1.7567,  0.4178],\n",
      "        [ 1.1351, -2.4262,  0.6793],\n",
      "        [ 0.9483, -1.1600,  0.1658],\n",
      "        [-0.2322, -1.5101,  0.2030],\n",
      "        [-0.1339, -1.5849,  0.4239],\n",
      "        [-0.2232, -1.6648,  0.3389],\n",
      "        [-0.3771, -1.6716,  0.3762],\n",
      "        [ 1.8918, -2.2451,  0.7373]])\n",
      "Test Loss: 1.0064, Test Accuracy: 0.5523\n",
      "Results saved to rnn_classification_results.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing with RNN model...\")\n",
    "rnn_model, rnn_accuracy, rnn_loss = process_data_with_splits(model_type='rnn', train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca9f16b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of models:\n",
      "RNN - Accuracy: 0.5523, Loss: 1.0064\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComparison of models:\")\n",
    "print(f\"RNN - Accuracy: {rnn_accuracy:.4f}, Loss: {rnn_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23750444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5242-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
